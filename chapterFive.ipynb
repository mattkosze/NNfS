{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 5: Loss Functions",
   "id": "e7b7de03f60754db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T00:24:08.945104Z",
     "start_time": "2025-06-17T00:24:08.935630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy"
   ],
   "id": "1af40dab532e829e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Categorical Cross-Entropy Loss\n",
    "\n",
    "If we were doing linear regression and modeling a regression line, then we'd be looking at mean squared error (MSE) right now. But, because we're on classification right now, we're looking at categorical cross-entropy instead. That is a mouthful. \n",
    "\n",
    "Categorical cross-entropy is used to explicitly compare a ground-truth probability (called \"y\" or \"targets\") with some predicted distribution (called \"y-hat\" or \"predictions\").\n",
    "\n",
    "$$\n",
    "L_{i} = -{\\sum_{j} y_{i,j}log(\\hat{y}_{i,j})}\n",
    "$$\n",
    "\n",
    "That is, where L(i) denotes sample's loss value, i is the i-th sample from the set, j is the label/output index, y denotes the target values, and y-hat denotes the predicted values. \n",
    "\n",
    "We'll be able to simplify this down to:\n",
    "\n",
    "$$\n",
    "L_{i} = -log(\\hat{y}_{i,k})\n",
    "$$\n",
    "\n",
    "Where L(i) denotes the sample's loss value, i is the i-th sample in the set, k is the index of the target label, y denotes the target values, and y-hat denotes the predictions.\n",
    "\n",
    "We encode desired outcomes using something called \"one-hot\" encoding, which is basically where the ground truth value is given a 1, and all other values are assigned a 0. \n",
    "\n",
    "So, if we have three options to choose from [a, b, c] and we want the ground truth to be \"b\", then we make the target list be [0, 1, 0]. Say that our softmax outputs [0.3, 0.4, 0.3], then our categorical cross-entropy function becomes:\n",
    "\n",
    "$$\n",
    "L_{i} = -(0 * log(0.3) + 1 * log(0.4) + 0 * log(0.3))\n",
    "$$ \n",
    "\n",
    "Which, when we have one hot encoded data, further simplifies to:\n",
    "\n",
    "$$\n",
    "L_{i} = -(log(0.4))\n",
    "$$\n",
    "\n",
    "Therefore proving why we can simplify the categorical cross-entropy loss function to just L(i) = -log(softmax_output[argmax(softmax_output))\n",
    "\n",
    "Below, we'll implement this using python to show it in code."
   ],
   "id": "2242bef35b3db252"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "softmax_output = [0.3, 0.4, 0.3]\n",
    "target_output = [0, 1, 0]\n",
    "\n",
    "# Here we're just doing (target[x] * softmax[x]) for each\n",
    "loss = -(target_output[0]*math.log(softmax_output[0]) + target_output[1]*math.log(softmax_output[1]) + target_output[2]*math.log(softmax_output[2]))\n",
    "\n",
    "print(f\"Loss out of the 'complex' version: {loss}\")\n",
    "\n",
    "# Here we just simplify it down to the only one which is true in our one-hot encoding\n",
    "loss = -(target_output[1]*math.log(softmax_output[1]))\n",
    "print(f\"Loss out of simplified version: {loss}\")"
   ],
   "id": "99489e096ab74a46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see: they're identical!!!\n",
    "\n",
    "Now, let's do an example: one in which we're processing the outputs to find a categorical cross-entry for each sample in the batch and ultimately for the batch as a whole. Let's do so below!  "
   ],
   "id": "fabb5350e896f4cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We'll be using some placeholder data here.\n",
    "softmax_outputs = np.array([[0.6, 0.2, 0.2],\n",
    "                            [0.3, 0.5, 0.2],\n",
    "                            [0.1, 0.1, 0.8]])\n",
    "\n",
    "# We'll also assign ground truths to go on\n",
    "class_targets = [0, 1, 2]\n",
    "\n",
    "# Now we'll roll all of these up into one to calculate the loss per sample\n",
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "print(f\"The c.ce.l. for each batch is: {neg_log}\")\n",
    "\n",
    "#Here we can leverage np.mean which just does sum(list x)/len(list x) in an easy to call method.\n",
    "average_loss = np.mean(neg_log)\n",
    "print(f\"The average c.ce.l is: {average_loss}\")"
   ],
   "id": "90e1da287d0db21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "However, as we can see, data can be sparsely encoded or one-hot encoded!! In the former case that means looking at [1] for each row with the number referencing the index of the ground truth, whereas in the latter case it would mean [0, 1, 0]. \n",
    "\n",
    "As such, our loss must be calculated differently depending on the way in which the target data is encoded. "
   ],
   "id": "897fae622493961f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4db0bbf7dd98299f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We'll be using some placeholder data here.\n",
    "softmax_outputs = np.array([[0.6, 0.2, 0.2],\n",
    "                            [0.3, 0.5, 0.2],\n",
    "                            [0.1, 0.1, 0.8]])\n",
    "\n",
    "# We'll also assign ground truths to go on\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 1]])\n",
    "\n",
    "# If there are sparsely-encoded labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "elif len(class_targets.shape) == 2:\n",
    "    confidences = np.sum(softmax_outputs * class_targets, axis=1)\n",
    "    \n",
    "# Losses\n",
    "neg_log = -np.log(confidences)\n",
    "avg_loss = np.mean(neg_log)\n",
    "\n",
    "print(avg_loss)"
   ],
   "id": "33d35e7a52325c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There's rightfully one more thing to point out here -- that the softmax can produce outputs as 1 or 0. We're specifically worried about the latter of the two, because log(0) is undefined. \n",
    "\n",
    "And the truthful answer is... there's not a really ideal way of solving it. But there is one which is commonly used: clipping. What that basically means is that you reduce your bounds from being 0 to 1 to now being from 1-e7 to (1-(1-e7)). It doesn't need to exactly be clipped by 1-e7, but by a very small non-zero value which will have a negligible impact on your model but prevent the log(0) error. We do so using the np.clip() method in numpy.    "
   ],
   "id": "ad2ea832ad0cf328"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T00:23:27.753505Z",
     "start_time": "2025-06-17T00:23:27.743670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We'll be using some placeholder data here.\n",
    "softmax_outputs = np.array([[0.6, 0.2, 0.2],\n",
    "                            [0.3, 0.5, 0.2],\n",
    "                            [0.1, 0.1, 0.8]])\n",
    "\n",
    "# We'll also assign ground truths to go on\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 1]])\n",
    "\n",
    "\n",
    "lossFunction = CategoricalCrossEntropy()\n",
    "loss = lossFunction.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ],
   "id": "ec36308f8e147386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47570545188004854\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll combine everything that's been done up to this point. We'll also add a measure to quantify precision/accuracy, which is how often it correctly predicts the right class.",
   "id": "4228c55f95eb1a08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T00:34:29.395953Z",
     "start_time": "2025-06-17T00:34:29.390233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Let's generate some data quickly.\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Now to initialize the neural network.\n",
    "Dense1 = DenseLayer(2,3)\n",
    "Dense2 = DenseLayer(3,3)\n",
    "Activation1 = ReLU()\n",
    "Activation2 = SoftMax()\n",
    "Loss = CategoricalCrossEntropy()\n",
    "\n",
    "# Now to do a forward pass through the whole model\n",
    "Dense1.forward(X)\n",
    "Activation1.forward(Dense1.output)\n",
    "Dense2.forward(Activation1.output)\n",
    "Activation2.forward(Dense2.output)\n",
    "\n",
    "# Now to quantify the predictions so we can figure out our accuracy\n",
    "predictions = np.argmax(Activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(y == predictions)\n",
    "\n",
    "# Lastly, we can calculate our loss in this batch.\n",
    "loss = Loss.calculate(Activation2.output, y)\n",
    "\n",
    "print(f\"The output layer produced: {Activation2.output[:5]}\")\n",
    "print(f\"The loss is: {loss}\")\n",
    "print(f\"The accuracy is: {accuracy}\")"
   ],
   "id": "1cdaef71d723e67e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output layer produced: [[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "The loss is: 1.0986104011535645\n",
      "The accuracy is: 0.34\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we actually can see how well the model we've compiled has been working! Now, we should be all set to take steps which will use the above to optimize our model!!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukieła!"
   ],
   "id": "1acb3956771a8d4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
