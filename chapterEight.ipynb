{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 8: Gradients + Partial Derivatives + Chain Rule \n",
    "\n",
    "Now, we've (well, I skipped, but the book did have it) talked about some elementary calculus. Now, we can do stuff that moreso focuses on the kind of things we'll need for our journey here.\n",
    "\n",
    "## Section 1: Partial Derivatives\n",
    "\n",
    "Partial derivatives are step one in our journey. A partial derivative measures how much impact a single input has on a function's output. As we're doing so, what that is really is just turns into a derivative with respect to the input. The partial derivative is the singular equation, and the full multivariate function's derivative consists of a set of equations called the gradient. Simply put, the gradient is a vector of the size of inputs containing partial derivative solutions with respect to each of the inputs. \n",
    "\n",
    "Calculating the partial derivative of a sum is super straightforward! All you need to do is calculate it like a regular derivative but instead go and set all other inputs as constants. I don't need to show an example here.\n",
    "\n",
    "For multiplication, it's very similar, except you must remember that you can make use of being able to take constants out of a derivative. I'll throw you a brain teaser to check your skills: what is the derivative of the following equation with respect to x?\n",
    "$$\n",
    "f(x,y,z) = 3x^{3}z - y^{2} + 5z + 2yz\n",
    "$$  \n",
    "Now, if you don't understand how to get to the following, you should review your Calculus:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(x,y,z) = 9x^{2}z\n",
    "$$ \n",
    "\n",
    "One more partial derivative to review here is that of the max. The max function returns the greatest input. So, if we take a look at the function:  \n",
    "$$\n",
    "f = max(x,y) \\rightarrow \\frac{d}{dx} = 1(x>y)\n",
    "$$\n",
    "The above will happen where x>y returns a 1 if true and 0 if false, so therefore the d/dx is 1 if true, 0 if false. \n",
    "\n",
    "The reason we go over this is because it relates to our ReLU function, where all it's technically doing is max(x, 0), therefore meaning that it's derivative with respect to x is just 1(x>0), which will also be 1 if true, 0 if false.\n",
    "\n",
    " Pay attention to the above, where we use the $\\partial$ operator when there is more than a single parameter and we are trying to get the partial derivative. On the other hand, we use the d operator when there is only one operator and we want the whole derivative.\n",
    " \n",
    "## Section 2: Gradients\n",
    "\n",
    "The gradient is a vector comprised of all the partial derivatives of a function. For example, if we take our function $f(x,y,z) = 3x^{2}z - y^{2} + 5z + 2yz$ then the gradient for it would be:\n",
    "$$\n",
    "\\nabla f(x,y,z)\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x} f(x,y,z) \\\\\n",
    "\\frac{\\partial}{\\partial y} f(x,y,z) \\\\\n",
    "\\frac{\\partial}{\\partial z} f(x,y,z)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "9x^{2}z \\\\\n",
    "-2y + 2z \\\\\n",
    "3x^{3} + 5 + 2y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As you can see, it's denoted by the $\\nabla$ and it simply a matrix where every row corresponds to the partial derivative of the function with respect to the corresponding variable. \n",
    "\n",
    "In the long run, we'll be using both derivatives and these gradients to perform gradient descent using the chain rule -- which is the \"backward pass\" of model training.  \n",
    "\n",
    "## Section 3: The Chain Rule\n",
    "\n",
    "During our forward passes, we are continuously using our outputs to funnel into our next step. For example, if we carry out the following:\n",
    "$$\n",
    "z = f(x) \\\\\n",
    "y = g(x)\n",
    "$$  \n",
    "Then we could rewrite it as $y = g(f(x))$. In this sense, it's nearly recursive in the sense it generally goes $output = outputActivation(dense2(dense1activation(dense1(inputs).output).output).output$, if that makes sense? That is effectively chaining.\n",
    "\n",
    "The chain rule is stated below:\n",
    "$$\n",
    "1. \\frac{d}{dx}[(f(x))^{n}] = n(f(x))^{n-1} * f'(x) \\\\\n",
    "2. \\frac{d}{dx}[f(g(x))] = f'(g(x)) * g'(x)\n",
    "$$\n",
    "\n",
    "I could bore you with more calculus and an example of the chain rule. But I don't think that's necessary and within the scope of my encapsulation of this book. \n",
    "\n",
    "Our next chapter actually gets to something really interesting: backpropagation. That makes use of all the Calculus (much of which I've omitted here) that has been covered in chapter eight of this book.\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça!\n"
   ],
   "id": "9251ef7db7526b7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
