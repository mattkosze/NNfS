{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 10: Optimizers",
   "id": "75b1bfe72c68755f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-20T14:51:59.902096Z",
     "start_time": "2025-06-20T14:51:59.897508Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from timeit import timeit\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy, SoftMaxCategoricalCrossEntropy, SGD"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We spent chapter nine learning about how we can calculate and apply the gradients to adjust weights & biases to ultimately reduce loss. What we ended up doing was subtracting a fraction of the gradient for each weight and bias parameter -- and that is called stochastic gradient descent (SGD). Most optimizer that we use are actually just modifications in the implementation of SGD.\n",
    "\n",
    "## Section 1: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Anyone who's heard about optimizers before has probably heard many names -- seemingly used interchangeably -- including:\n",
    "- Stochastic Gradient Descent or SGD.\n",
    "- Vanilla Gradient Descent, Gradient Descent or GD, Batch Gradient Descent or BGD.\n",
    "- Mini-batch Gradient Descent or MBGD.\n",
    "\n",
    "However: these are not the same. SGD has historically referred to an optimizer that fits a single sample at a time. Meanwhile, BGD is an optimizer used to fit a whole dataset at once. Lastly, MBGD is an optimizer used to fit slices to a dataset, which we'd call batches in our context.\n",
    "\n",
    "As a general rule of thumb, we call slices of data **batches**. However, historically, these same slices have been referred to as **mini-batches** in the context of SGD. With that said, the field has evolved and the two are now used interchangeably to the point where we actually think of the SGD optimizer as one that assumes a batch of data.\n",
    "\n",
    "In the case of SGD, we need to choose a learning rate. From that, we then subtract $\\text{learning_rate} \\cdot \\text{paramater_gradients}$ from the actual values. \n",
    "\n",
    "We'll walk through creating an example SGD class below -- but I've already made one and added it to the class.py file :)."
   ],
   "id": "ab47c2c624a27552"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ExampleSGD():\n",
    "    # Initialize the class\n",
    "    def __init__(self, lr=1.0):\n",
    "        # Store the learning rate; just 1 if not specified\n",
    "        self.lr = lr\n",
    "        \n",
    "    # Method to update the parameters after a backward pass\n",
    "    def updateParams(self, layer):\n",
    "        # Update values according to: -lr * parameter_gradients\n",
    "        layer.weights += -self.lr * layer.dweights\n",
    "        layer.bias += -self.lr * layer.dbiases"
   ],
   "id": "6afb1ce69e52d541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above is an early version of our SGD class! Now, to use it, we have to instantiate the object by doing \"optimizer = SGD()\" and then doing \"optimizer.updateParams(denseX)\" for each hidden layer X in our model.\n",
    "\n",
    "Again, the above is code cell is just an example implementation for convenience; we can just directly reference the SGD() class from our classes.py.\n",
    "\n",
    "With this built out, we can begin training our model in repeated iterations called epochs. An epoch simply means a full pass through all the training data. So, let's implement our model to trainable in epochs by leveraging looping. "
   ],
   "id": "78e0506eb7feeb0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:59:19.633950Z",
     "start_time": "2025-06-20T14:59:16.864393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating some training data used the spiral_data function\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create dense layer with 2 input features and 64 output features\n",
    "dense1 = DenseLayer(2, 64)\n",
    "\n",
    "# Use a relu activation\n",
    "activation1 = ReLU()\n",
    "\n",
    "# Create a dense layer for our output with 64 as an input and 3 as an output\n",
    "dense2 = DenseLayer(64, 3)\n",
    "\n",
    "# Use a softmax combined with ccel. for our output \n",
    "activationLoss = SoftMaxCategoricalCrossEntropy()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = SGD()\n",
    "\n",
    "# Create the loop that trains our model in epochs\n",
    "for epoch in range(10000):\n",
    "    # Perform the forward pass, as shown previously\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = activationLoss.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    predictions = np.argmax(activationLoss.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, accuracy: {accuracy: .3f}, loss: {loss: .3f}\")\n",
    "        \n",
    "    # Perform the backward pass\n",
    "    activationLoss.backward(activationLoss.output, y)\n",
    "    dense2.backward(activationLoss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Use the optimizer and update the weights and biases\n",
    "    optimizer.updateParams(dense1)\n",
    "    optimizer.updateParams(dense2)"
   ],
   "id": "f3bf0d52a9a085d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy:  0.320, loss:  1.099\n",
      "epoch: 100, accuracy:  0.427, loss:  1.078\n",
      "epoch: 200, accuracy:  0.427, loss:  1.066\n",
      "epoch: 300, accuracy:  0.430, loss:  1.064\n",
      "epoch: 400, accuracy:  0.433, loss:  1.062\n",
      "epoch: 500, accuracy:  0.430, loss:  1.060\n",
      "epoch: 600, accuracy:  0.433, loss:  1.056\n",
      "epoch: 700, accuracy:  0.437, loss:  1.052\n",
      "epoch: 800, accuracy:  0.437, loss:  1.059\n",
      "epoch: 900, accuracy:  0.427, loss:  1.060\n",
      "epoch: 1000, accuracy:  0.403, loss:  1.060\n",
      "epoch: 1100, accuracy:  0.407, loss:  1.057\n",
      "epoch: 1200, accuracy:  0.420, loss:  1.050\n",
      "epoch: 1300, accuracy:  0.410, loss:  1.039\n",
      "epoch: 1400, accuracy:  0.400, loss:  1.028\n",
      "epoch: 1500, accuracy:  0.430, loss:  1.008\n",
      "epoch: 1600, accuracy:  0.423, loss:  1.015\n",
      "epoch: 1700, accuracy:  0.457, loss:  0.998\n",
      "epoch: 1800, accuracy:  0.450, loss:  0.982\n",
      "epoch: 1900, accuracy:  0.413, loss:  0.993\n",
      "epoch: 2000, accuracy:  0.483, loss:  0.975\n",
      "epoch: 2100, accuracy:  0.480, loss:  0.949\n",
      "epoch: 2200, accuracy:  0.517, loss:  0.933\n",
      "epoch: 2300, accuracy:  0.503, loss:  0.926\n",
      "epoch: 2400, accuracy:  0.533, loss:  0.893\n",
      "epoch: 2500, accuracy:  0.530, loss:  0.916\n",
      "epoch: 2600, accuracy:  0.553, loss:  0.863\n",
      "epoch: 2700, accuracy:  0.510, loss:  0.927\n",
      "epoch: 2800, accuracy:  0.557, loss:  0.859\n",
      "epoch: 2900, accuracy:  0.580, loss:  0.860\n",
      "epoch: 3000, accuracy:  0.530, loss:  0.828\n",
      "epoch: 3100, accuracy:  0.557, loss:  0.855\n",
      "epoch: 3200, accuracy:  0.597, loss:  0.821\n",
      "epoch: 3300, accuracy:  0.567, loss:  0.816\n",
      "epoch: 3400, accuracy:  0.587, loss:  0.822\n",
      "epoch: 3500, accuracy:  0.503, loss:  1.007\n",
      "epoch: 3600, accuracy:  0.543, loss:  0.821\n",
      "epoch: 3700, accuracy:  0.570, loss:  0.864\n",
      "epoch: 3800, accuracy:  0.653, loss:  0.746\n",
      "epoch: 3900, accuracy:  0.603, loss:  0.779\n",
      "epoch: 4000, accuracy:  0.620, loss:  0.770\n",
      "epoch: 4100, accuracy:  0.583, loss:  0.795\n",
      "epoch: 4200, accuracy:  0.590, loss:  0.785\n",
      "epoch: 4300, accuracy:  0.597, loss:  0.782\n",
      "epoch: 4400, accuracy:  0.607, loss:  0.834\n",
      "epoch: 4500, accuracy:  0.577, loss:  0.805\n",
      "epoch: 4600, accuracy:  0.613, loss:  0.766\n",
      "epoch: 4700, accuracy:  0.577, loss:  0.799\n",
      "epoch: 4800, accuracy:  0.593, loss:  0.777\n",
      "epoch: 4900, accuracy:  0.593, loss:  0.763\n",
      "epoch: 5000, accuracy:  0.607, loss:  0.771\n",
      "epoch: 5100, accuracy:  0.590, loss:  0.817\n",
      "epoch: 5200, accuracy:  0.597, loss:  0.759\n",
      "epoch: 5300, accuracy:  0.667, loss:  0.712\n",
      "epoch: 5400, accuracy:  0.603, loss:  0.784\n",
      "epoch: 5500, accuracy:  0.627, loss:  0.749\n",
      "epoch: 5600, accuracy:  0.623, loss:  0.798\n",
      "epoch: 5700, accuracy:  0.640, loss:  0.736\n",
      "epoch: 5800, accuracy:  0.623, loss:  0.761\n",
      "epoch: 5900, accuracy:  0.633, loss:  0.741\n",
      "epoch: 6000, accuracy:  0.640, loss:  0.749\n",
      "epoch: 6100, accuracy:  0.463, loss:  1.281\n",
      "epoch: 6200, accuracy:  0.637, loss:  0.749\n",
      "epoch: 6300, accuracy:  0.653, loss:  0.725\n",
      "epoch: 6400, accuracy:  0.643, loss:  0.684\n",
      "epoch: 6500, accuracy:  0.643, loss:  0.741\n",
      "epoch: 6600, accuracy:  0.643, loss:  0.718\n",
      "epoch: 6700, accuracy:  0.650, loss:  0.703\n",
      "epoch: 6800, accuracy:  0.650, loss:  0.722\n",
      "epoch: 6900, accuracy:  0.663, loss:  0.715\n",
      "epoch: 7000, accuracy:  0.670, loss:  0.662\n",
      "epoch: 7100, accuracy:  0.653, loss:  0.749\n",
      "epoch: 7200, accuracy:  0.613, loss:  0.777\n",
      "epoch: 7300, accuracy:  0.683, loss:  0.669\n",
      "epoch: 7400, accuracy:  0.657, loss:  0.691\n",
      "epoch: 7500, accuracy:  0.667, loss:  0.665\n",
      "epoch: 7600, accuracy:  0.700, loss:  0.650\n",
      "epoch: 7700, accuracy:  0.680, loss:  0.653\n",
      "epoch: 7800, accuracy:  0.653, loss:  0.713\n",
      "epoch: 7900, accuracy:  0.687, loss:  0.648\n",
      "epoch: 8000, accuracy:  0.683, loss:  0.653\n",
      "epoch: 8100, accuracy:  0.643, loss:  0.653\n",
      "epoch: 8200, accuracy:  0.677, loss:  0.642\n",
      "epoch: 8300, accuracy:  0.663, loss:  0.650\n",
      "epoch: 8400, accuracy:  0.660, loss:  0.629\n",
      "epoch: 8500, accuracy:  0.673, loss:  0.656\n",
      "epoch: 8600, accuracy:  0.683, loss:  0.640\n",
      "epoch: 8700, accuracy:  0.663, loss:  0.623\n",
      "epoch: 8800, accuracy:  0.680, loss:  0.648\n",
      "epoch: 8900, accuracy:  0.687, loss:  0.638\n",
      "epoch: 9000, accuracy:  0.690, loss:  0.601\n",
      "epoch: 9100, accuracy:  0.660, loss:  0.630\n",
      "epoch: 9200, accuracy:  0.687, loss:  0.644\n",
      "epoch: 9300, accuracy:  0.687, loss:  0.628\n",
      "epoch: 9400, accuracy:  0.690, loss:  0.583\n",
      "epoch: 9500, accuracy:  0.693, loss:  0.618\n",
      "epoch: 9600, accuracy:  0.680, loss:  0.627\n",
      "epoch: 9700, accuracy:  0.680, loss:  0.652\n",
      "epoch: 9800, accuracy:  0.680, loss:  0.640\n",
      "epoch: 9900, accuracy:  0.697, loss:  0.621\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now making models that are learning and performing better on their dataset -- which we can clearly see by increasing accuracy and decreasing loss. However, our accuracy seems to get stuck just below the ~.70 mark with about ~.62 loss. That tells us the model may have reached a local minimum, which we'll talk about soon. That tells us that adding epochs likely won't be very helpful at this point, and we need to work on our optimizer. The first thing we can change is the learning rate, so that's what we'll look at next!\n",
    "\n",
    "## Section 2: Learning Rates\n",
    "\n",
    "We want to apply a fraction of the gradients to the parameters in order to descend the loss value. Typically, we don't apply the full gradient value (which would just be the slope of the tangent line) because these values will typically be too large to produce meaningful improvements. Instead, we want to perform small steps, calculating the gradient and updating parameters by a negative fraction of this gradient. These small steps allow us to ensure that we are following the direction of the steepest descent -- but these can be too small, causing learning stagnation.\n",
    "\n",
    "When the learning rate is too small, the update to the parameters are too small the model may get stuck in a local minimum as opposed to actually finding the global minimum. So, how do we know if our model is at the global minimum? We know it is so when the loss reaches as close to 0 as possible -- but we frequently never reach 0 in practice, as that may cause overfitting (a topic which I assume will be talked about in a later chapter). \n",
    "\n",
    "Learning rate in itself is not enough -- you may still be getting stuck in local minimums no matter what you set it to. So, for that reason, we must introduce momentum. Momentum, in an optimizer, adds to the gradient what we, in the physical world, would call intertia. For example, if we throw a ball uphill, then with enough force or a small enough hill, the ball can roll over the crest of the hill and onto the other side.\n",
    "\n",
    "These parameters in the optimizer (such as the learning rate or momentum) are referred to as hyperparameters.\n",
    "\n",
    "If the learning rate is too big the model may start jumping around during SGD, caused by the amount of gradient applied being too large. At the extreme, this may cause a gradient explosion. A gradient explosion is where the parameter updates cause the function's output to rise instead of fall, causing the gradient and loss to increase with each step. \n",
    "\n",
    "Ultimately, choosing the correct hyperparameters will enable you to speed up the learning process and save yourself money and time. It's typically best to start with the optimizer defaults, perform a few steps, and then observe the training process when tuning different settings. However, it's always useful to have some system that actively tunes your hyperparameters even during training. One way that we do this is by using learning rate decay. \n",
    "\n",
    "## Section 3: Learning Rate Decay   "
   ],
   "id": "58eb1f6d43aac0ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "68c5fd4c40903e61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
