{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 11: Testing with Out of Sample Data\n",
    "\n",
    "The complexity of neural networks is simultaneously their biggest strength and weakness. With enough neurons, a model can easily memorize a dataset; but it cannot generalize the data with too few of them.\n",
    "\n",
    "If you remember, in the last chapter, we trained our model to an accuracy of nearing 95%. However, the question to ask is: how will this generalize across unseen data? With an accuracy of 95%, there is a significant possibility that our model has overfitted. Overfitting is when the model is effectively just memorizing the data without actually understanding it. A overfit model will do better on predicting data it has already seen, but typically much worse on unseen data.\n",
    "\n",
    "Without knowing if a model overfits the training data, we can't trust its results. For this reason, it is essential to have both training and testing data as separate sets for different purposes.\n",
    "\n",
    "Training data should be used for just that: training. On the other hand, testing data should be used separately to validate a model's performance after training. The idea is that this data will be withheld from the training process and will therefore provide a better indication of real-world performance.\n",
    "\n",
    "We can recognize overfitting when testing data results begin to diverge in trend from the training data. Performance against training data will typically be better, but a difference of over 10% usually means the model is likely overfitting. Modest overfitting is not a severe problem, but is still something to be minimized.\n",
    "\n",
    "Besides adding testing data, another method to prevent overfitting is to change your model size. If a model is not learning, you should try a larger model. On the other hand, if the model is learning, but there is divergence between test and train, it may make sense to try a smaller model. Generally, the rule of thumb is to find the smallest possible model that still trains. \n",
    "\n",
    "This is a really short chapter, being only 7 pages in the book. Next is validation data!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça! "
   ],
   "id": "291c4e83ec227412"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
