{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 15: Dropout",
   "id": "8e5d1a019766020c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T17:58:39.242488Z",
     "start_time": "2025-06-22T17:58:38.886933Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from timeit import timeit\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy, SoftMaxCategoricalCrossEntropy, SGD, AdaGrad, RMSProp, Adam, DropoutLayer\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Yet another option for neural network regularization is adding a dropout layer. \n",
    "\n",
    "This is a kind of layer that disables some neurons while others remain unchanged. The idea here is similar, where it is meant to prevent a neural network from becoming too dependent on a particular neuron. Dropout also helps with the issue of co-adoption, which happens when certain neurons depend on the output values of other neurons and as a result don't learn the patterns on their own. \n",
    "\n",
    "The Dropout function works by randomly disabling neurons at a given rate during every forward pass, forcing the network to make accurate predictions with a random amount of functional neurons. It forces a model to use a greater number of neurons for the same purpose resulting in a higher likelihood of learning patterns in the data, versus just the data itself.\n",
    "\n",
    "## Section 1: The Forward Pass\n",
    "\n",
    "We carry out dropout by turning off certain neuron inputs at random, which we do by just zeroing their outputs. To do so, we use a filter which is an array of the same shape as the layer output but filled with numbers drawm from a Bernoulli distribution. To provide an exact definition: a Bernoulli distribution is a binary probability distribution where we can get a value of 1 with a probability of p and a value of 0 with a probability of q. For example, if we take a random value, let's call it $r_{i}$, from the distribution, then:\n",
    "$$\n",
    "P(r_{i} = 1) = p \\\\\n",
    "P(r_{i} = 0 = q = 1 - p = 1 - P(r_{i} = 1)\n",
    "$$\n",
    "If the probability of $r_{i}$ being 1 is p, then the probability of 0 being q is (1-p), therefore:\n",
    "$$\n",
    "r_{i} \\sim Bernoulli(p)\n",
    "$$\n",
    "From all of this, we can gather that the given $r_{i}$ is an equivalent of a value from the Bernoulli distribution with a probability p for this value to be 1. Finally, we are returned an array filled with values of 1 with a probability of p and values of 0 with a probability of q. We then apply this filter to the layer output that we're trying to add the dropout to.\n",
    "\n",
    "In our code, we only have one hyperparameter: the dropout rate. The dropout rate represents the percent of neurons in that layer to disable. For example, a dropout rate of 0.3 will mean that 30% of neurons are disabled at random during each forward pass.\n",
    "\n",
    "Let's demonstrate this is vanilla python: "
   ],
   "id": "5372ccae02ca3566"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T16:30:58.184904Z",
     "start_time": "2025-06-22T16:30:58.178464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dropoutRate = 0.3\n",
    "# Mock output for testing purposes\n",
    "exOutput = [0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73]\n",
    "\n",
    "# Repeat as many times as need to zero all necessary outputs\n",
    "while True:\n",
    "    # Randomly choose index\n",
    "    index =random.randint(0, len(exOutput)-1)\n",
    "    exOutput[index] = 0\n",
    "    \n",
    "    # Check the total amount of 0's \n",
    "    zeroD = 0\n",
    "    for value in exOutput:\n",
    "        if value == 0:\n",
    "            zeroD += 1\n",
    "            \n",
    "    # Check the zeroD / total ratio is equal to the dropout rate\n",
    "    if zeroD / len(exOutput) >= dropoutRate:\n",
    "        break\n",
    "\n",
    "print(exOutput)"
   ],
   "id": "38bd12721c18e70d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27, 0, 0.67, 0.99, 0, -0.37, -2.01, 1.13, 0, 0.73]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That's the basic idea of dropout; pretty simple right? Yes, but there's a simpler way of doing this!\n",
    "\n",
    "We can leverage the np.random.binomial() method. The binomial method is only different from the bernoulli distribution in one way -- it adds a parameter n. N is the number of concurrent experiments and returns the number of successes from these n experiments. \n",
    "\n",
    "Np.random.binomial() takes in the parameters n, p, and size. This is where n is how many experiments to run for that sample, p is the probability for an experiment result to be 1, and size is amount of times you run the experiments with n samples each.\n",
    "\n",
    "So, let's do it in Numpy now!"
   ],
   "id": "2a2078807586b024"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T16:49:27.879689Z",
     "start_time": "2025-06-22T16:49:27.869509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hard wire our dropout rate\n",
    "dropoutRate = 0.3\n",
    "# Mock output for testing purposes\n",
    "exOutput = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "\n",
    "exOutput *= np.random.binomial(1, 1-dropoutRate, exOutput.shape)\n",
    "\n",
    "print(exOutput)"
   ],
   "id": "2c12f9cdfa4e4f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27 -1.03  0.67  0.99  0.   -0.37 -2.01  1.13 -0.    0.73]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Something to note here is that, in the book's presentation, the dropout rate is the percent of neurons to drop. On the other hand, in frameworks like PyTorch, they represent the percent of neurons to keep.\n",
    "\n",
    "So, we're on the right track with dropout, but there's one more thing to add: scaling the data. When we use a dropout, it sometimes results in the data having mismatched output sizes during training and inference. To resolve this, we also scale the dropout.\n",
    "\n",
    "We do this as such:\n",
    "```\n",
    "exOutput *= np.random.binomial(1, 1-dropoutRate, exOutput.shape) / (1-dropoutRate)\n",
    "```\n",
    "\n",
    "This way, our data output sizes are scaled back up and there is no longer an imbalance!\n",
    "\n",
    "## Section 2: The Backward Pass\n",
    "\n",
    "This section will very briefly show the derivative of the dropout function. Let's denote dropout as $D_{r}$, as such:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_{i}} D_{r_{i}} = \\frac{r_{i}}{1-q} \n",
    "$$\n",
    "\n",
    "That's really it. I've kept that super short because that's really all the detail you need. Now we can implement it in a class. I'll do an example class here and implement the full class in our classes.py file."
   ],
   "id": "b2ed2310835bcf3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExampleDropoutLayer:\n",
    "    # Method to initialize\n",
    "    def __init__(self, rate):\n",
    "        # Remember, we invert the rate\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    # Forward pass method\n",
    "    def forward(self, inputs):\n",
    "        # Save the inputs\n",
    "        self.inputs = inputs\n",
    "        # Create mask and scale it\n",
    "        self.binaryMask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        # Apply the mask to the outputs\n",
    "        self.output = inputs * self.binaryMask\n",
    "        \n",
    "    # Backward pass method\n",
    "    def backward(self, dvalues):\n",
    "        # The gradient\n",
    "        self.dinputs = dvalues * self.binaryMask"
   ],
   "id": "2aa7e9fb80ca8fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that's all we need for our dropout layer. We just slide is in between our outputs and inputs of the following layer and it works plug-and-play. Let's use this in our model now!",
   "id": "2e920ddb70d24fff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:14:21.278763Z",
     "start_time": "2025-06-22T18:09:04.343566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating some training data used the spiral_data function\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "\n",
    "# Create dense layer with 2 input features and 64 output features\n",
    "# NEW: changed the layer size from 64 to 512 to improve accuracy\n",
    "dense1 = DenseLayer(2, 512, weightl2=5e-4, biasl2=5e-4)\n",
    "\n",
    "# Use a relu activation\n",
    "activation1 = ReLU()\n",
    "\n",
    "# NEW: Create the dropout layer\n",
    "dropout1 = DropoutLayer(0.1)\n",
    "\n",
    "# Create a dense layer for our output with 64 as an input and 3 as an output\n",
    "# NEW: changed the layer size from 64 to 512 to improve accuracy\n",
    "dense2 = DenseLayer(512, 3)\n",
    "\n",
    "# Use a softmax combined with ccel. for our output \n",
    "activationLoss = SoftMaxCategoricalCrossEntropy()\n",
    "\n",
    "# Initialize optimizer as Adagrad with a decay\n",
    "optimizer = Adam(lr=0.05, decay=5e-7)\n",
    "\n",
    "# Create the loop that trains our model in epochs\n",
    "for epoch in range(10000):\n",
    "    # Perform the forward pass, as shown previously\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "    dense2.forward(dropout1.output)\n",
    "    # calculate dataLoss, regLoss, and then add for total loss\n",
    "    dataLoss = activationLoss.forward(dense2.output, y)\n",
    "    regLoss = activationLoss.loss.regularizationLoss(dense1) + activationLoss.loss.regularizationLoss(dense2)\n",
    "    loss = dataLoss + regLoss\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    predictions = np.argmax(activationLoss.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, accuracy: {accuracy: .3f}, loss: {loss: .3f}, dLoss: {dataLoss}, rLoss: {regLoss}, lr: {optimizer.lr_curr}\")\n",
    "        \n",
    "    # Perform the backward pass\n",
    "    activationLoss.backward(activationLoss.output, y)\n",
    "    dense2.backward(activationLoss.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Use the optimizer and update the weights and biases\n",
    "    optimizer.preUpdateParams()\n",
    "    optimizer.updateParams(dense1)\n",
    "    optimizer.updateParams(dense2)\n",
    "    optimizer.postUpdateParams()"
   ],
   "id": "b4a12ed1463ba62f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy:  0.289, loss:  1.099, dLoss: 1.0987270628564938, rLoss: 5.2330243903968354e-05, lr: 0.05\n",
      "epoch: 100, accuracy:  0.714, loss:  0.735, dLoss: 0.6753092698186839, rLoss: 0.05958928210316922, lr: 0.04999752512250644\n",
      "epoch: 200, accuracy:  0.766, loss:  0.642, dLoss: 0.5676832829999394, rLoss: 0.07415277804248334, lr: 0.04999502549496326\n",
      "epoch: 300, accuracy:  0.778, loss:  0.630, dLoss: 0.5510411084907014, rLoss: 0.07868330977248542, lr: 0.049992526117345455\n",
      "epoch: 400, accuracy:  0.811, loss:  0.566, dLoss: 0.4882249400259358, rLoss: 0.07795775180196346, lr: 0.04999002698961558\n",
      "epoch: 500, accuracy:  0.826, loss:  0.568, dLoss: 0.49081368857481905, rLoss: 0.07754818539347334, lr: 0.049987528111736124\n",
      "epoch: 600, accuracy:  0.837, loss:  0.541, dLoss: 0.46610160442782295, rLoss: 0.07444942020261587, lr: 0.049985029483669646\n",
      "epoch: 700, accuracy:  0.827, loss:  0.541, dLoss: 0.45862855836980543, rLoss: 0.08273262613898158, lr: 0.049982531105378675\n",
      "epoch: 800, accuracy:  0.844, loss:  0.520, dLoss: 0.44528511995944337, rLoss: 0.07438760321956649, lr: 0.04998003297682575\n",
      "epoch: 900, accuracy:  0.834, loss:  0.537, dLoss: 0.46685006703708615, rLoss: 0.07000955337516321, lr: 0.049977535097973466\n",
      "epoch: 1000, accuracy:  0.840, loss:  0.518, dLoss: 0.4490127990422152, rLoss: 0.06854505285798669, lr: 0.049975037468784345\n",
      "epoch: 1100, accuracy:  0.846, loss:  0.498, dLoss: 0.43040970674492246, rLoss: 0.06781912728464139, lr: 0.049972540089220974\n",
      "epoch: 1200, accuracy:  0.835, loss:  0.534, dLoss: 0.4650823735142461, rLoss: 0.06845598909734019, lr: 0.04997004295924593\n",
      "epoch: 1300, accuracy:  0.851, loss:  0.478, dLoss: 0.4111247246017737, rLoss: 0.06687949329169607, lr: 0.04996754607882181\n",
      "epoch: 1400, accuracy:  0.839, loss:  0.499, dLoss: 0.43521007637489795, rLoss: 0.0637400280198017, lr: 0.049965049447911185\n",
      "epoch: 1500, accuracy:  0.855, loss:  0.475, dLoss: 0.41139806158775627, rLoss: 0.06359796237384702, lr: 0.04996255306647668\n",
      "epoch: 1600, accuracy:  0.848, loss:  0.482, dLoss: 0.41937884554929605, rLoss: 0.06230097214976507, lr: 0.049960056934480884\n",
      "epoch: 1700, accuracy:  0.856, loss:  0.474, dLoss: 0.4052038936436993, rLoss: 0.0692063478135859, lr: 0.04995756105188642\n",
      "epoch: 1800, accuracy:  0.847, loss:  0.480, dLoss: 0.41581633591818157, rLoss: 0.06420179774712875, lr: 0.049955065418655915\n",
      "epoch: 1900, accuracy:  0.850, loss:  0.486, dLoss: 0.4245888354269036, rLoss: 0.061155997145848795, lr: 0.04995257003475201\n",
      "epoch: 2000, accuracy:  0.830, loss:  0.574, dLoss: 0.4729226693739697, rLoss: 0.10064955796107618, lr: 0.04995007490013731\n",
      "epoch: 2100, accuracy:  0.825, loss:  0.554, dLoss: 0.4642887998162749, rLoss: 0.08973754050228534, lr: 0.0499475800147745\n",
      "epoch: 2200, accuracy:  0.842, loss:  0.495, dLoss: 0.4139591802327877, rLoss: 0.08148414565595956, lr: 0.0499450853786262\n",
      "epoch: 2300, accuracy:  0.843, loss:  0.499, dLoss: 0.4231750656722628, rLoss: 0.07540684136328504, lr: 0.0499425909916551\n",
      "epoch: 2400, accuracy:  0.843, loss:  0.493, dLoss: 0.42290739963643764, rLoss: 0.07012640894547903, lr: 0.04994009685382384\n",
      "epoch: 2500, accuracy:  0.837, loss:  0.519, dLoss: 0.4522065831312842, rLoss: 0.06657841601312176, lr: 0.04993760296509512\n",
      "epoch: 2600, accuracy:  0.830, loss:  0.508, dLoss: 0.44409995023024035, rLoss: 0.06435338716017494, lr: 0.049935109325431604\n",
      "epoch: 2700, accuracy:  0.846, loss:  0.512, dLoss: 0.4495573637848954, rLoss: 0.06223716488630748, lr: 0.049932615934796004\n",
      "epoch: 2800, accuracy:  0.837, loss:  0.485, dLoss: 0.42300481149184754, rLoss: 0.061543343847904254, lr: 0.04993012279315098\n",
      "epoch: 2900, accuracy:  0.828, loss:  0.490, dLoss: 0.429504660700168, rLoss: 0.060285803078389114, lr: 0.049927629900459285\n",
      "epoch: 3000, accuracy:  0.847, loss:  0.510, dLoss: 0.44964240024273144, rLoss: 0.06023567348701294, lr: 0.049925137256683606\n",
      "epoch: 3100, accuracy:  0.842, loss:  0.471, dLoss: 0.41038415487104984, rLoss: 0.06069162351570437, lr: 0.04992264486178666\n",
      "epoch: 3200, accuracy:  0.843, loss:  0.492, dLoss: 0.432876313393286, rLoss: 0.05934284192520435, lr: 0.04992015271573119\n",
      "epoch: 3300, accuracy:  0.847, loss:  0.489, dLoss: 0.4313133225825858, rLoss: 0.05755971137201817, lr: 0.04991766081847992\n",
      "epoch: 3400, accuracy:  0.826, loss:  0.512, dLoss: 0.4536092570642563, rLoss: 0.05817207663280289, lr: 0.049915169169995596\n",
      "epoch: 3500, accuracy:  0.857, loss:  0.483, dLoss: 0.4253587301693345, rLoss: 0.05731849927439998, lr: 0.049912677770240964\n",
      "epoch: 3600, accuracy:  0.840, loss:  0.502, dLoss: 0.4447173307987335, rLoss: 0.057345818364934374, lr: 0.049910186619178794\n",
      "epoch: 3700, accuracy:  0.843, loss:  0.491, dLoss: 0.4326904720379419, rLoss: 0.05812353494672506, lr: 0.04990769571677183\n",
      "epoch: 3800, accuracy:  0.851, loss:  0.484, dLoss: 0.42523759373835257, rLoss: 0.05830555132020704, lr: 0.04990520506298287\n",
      "epoch: 3900, accuracy:  0.855, loss:  0.494, dLoss: 0.4376140003739139, rLoss: 0.05608329489488137, lr: 0.04990271465777467\n",
      "epoch: 4000, accuracy:  0.855, loss:  0.480, dLoss: 0.42379977264597035, rLoss: 0.05639248602423927, lr: 0.049900224501110035\n",
      "epoch: 4100, accuracy:  0.867, loss:  0.477, dLoss: 0.42047472515741835, rLoss: 0.05650790934879283, lr: 0.04989773459295174\n",
      "epoch: 4200, accuracy:  0.862, loss:  0.455, dLoss: 0.39942836521565805, rLoss: 0.055875188733033765, lr: 0.04989524493326262\n",
      "epoch: 4300, accuracy:  0.835, loss:  0.544, dLoss: 0.4853118532467355, rLoss: 0.05825883292850664, lr: 0.04989275552200545\n",
      "epoch: 4400, accuracy:  0.846, loss:  0.471, dLoss: 0.413725288776827, rLoss: 0.05766379070617101, lr: 0.04989026635914307\n",
      "epoch: 4500, accuracy:  0.848, loss:  0.489, dLoss: 0.43144185460933054, rLoss: 0.05752848332187694, lr: 0.04988777744463829\n",
      "epoch: 4600, accuracy:  0.836, loss:  0.488, dLoss: 0.4325603784295466, rLoss: 0.055771411518442074, lr: 0.049885288778453954\n",
      "epoch: 4700, accuracy:  0.864, loss:  0.462, dLoss: 0.40615200400246104, rLoss: 0.05547428434174352, lr: 0.049882800360552884\n",
      "epoch: 4800, accuracy:  0.855, loss:  0.472, dLoss: 0.4169248259181711, rLoss: 0.05465012067303648, lr: 0.04988031219089794\n",
      "epoch: 4900, accuracy:  0.824, loss:  0.527, dLoss: 0.46716272761396826, rLoss: 0.06014427643413895, lr: 0.049877824269451976\n",
      "epoch: 5000, accuracy:  0.831, loss:  0.511, dLoss: 0.45088848437701884, rLoss: 0.060598946943233555, lr: 0.04987533659617785\n",
      "epoch: 5100, accuracy:  0.849, loss:  0.495, dLoss: 0.43852238377761843, rLoss: 0.05683800112693357, lr: 0.04987284917103844\n",
      "epoch: 5200, accuracy:  0.857, loss:  0.470, dLoss: 0.4151427848119761, rLoss: 0.05463640772818181, lr: 0.04987036199399661\n",
      "epoch: 5300, accuracy:  0.848, loss:  0.476, dLoss: 0.4205117686973821, rLoss: 0.05581966367180975, lr: 0.04986787506501525\n",
      "epoch: 5400, accuracy:  0.843, loss:  0.506, dLoss: 0.45067411930940926, rLoss: 0.055313992074876375, lr: 0.04986538838405724\n",
      "epoch: 5500, accuracy:  0.846, loss:  0.500, dLoss: 0.44631610416342193, rLoss: 0.053499865530565634, lr: 0.049862901951085496\n",
      "epoch: 5600, accuracy:  0.836, loss:  0.491, dLoss: 0.43923180627593744, rLoss: 0.05202656254970668, lr: 0.049860415766062906\n",
      "epoch: 5700, accuracy:  0.861, loss:  0.463, dLoss: 0.40649167153032295, rLoss: 0.056295926977293284, lr: 0.0498579298289524\n",
      "epoch: 5800, accuracy:  0.850, loss:  0.478, dLoss: 0.4209093098321772, rLoss: 0.05685748290917657, lr: 0.04985544413971689\n",
      "epoch: 5900, accuracy:  0.851, loss:  0.495, dLoss: 0.42729338382797416, rLoss: 0.06801931941664355, lr: 0.049852958698319315\n",
      "epoch: 6000, accuracy:  0.853, loss:  0.483, dLoss: 0.42069088877974, rLoss: 0.06272822991478964, lr: 0.04985047350472258\n",
      "epoch: 6100, accuracy:  0.847, loss:  0.487, dLoss: 0.42917468482239624, rLoss: 0.057739741688179885, lr: 0.04984798855888967\n",
      "epoch: 6200, accuracy:  0.836, loss:  0.480, dLoss: 0.42396355139759156, rLoss: 0.05606362277283246, lr: 0.049845503860783506\n",
      "epoch: 6300, accuracy:  0.849, loss:  0.468, dLoss: 0.4145159582530386, rLoss: 0.05341814356862233, lr: 0.049843019410367055\n",
      "epoch: 6400, accuracy:  0.827, loss:  0.527, dLoss: 0.45127519073373007, rLoss: 0.07594993448260737, lr: 0.04984053520760327\n",
      "epoch: 6500, accuracy:  0.839, loss:  0.506, dLoss: 0.4399191898665861, rLoss: 0.06583155274023655, lr: 0.049838051252455155\n",
      "epoch: 6600, accuracy:  0.851, loss:  0.477, dLoss: 0.41546599431944864, rLoss: 0.06129137103815156, lr: 0.049835567544885655\n",
      "epoch: 6700, accuracy:  0.844, loss:  0.480, dLoss: 0.4223845527584401, rLoss: 0.05761298000592844, lr: 0.04983308408485778\n",
      "epoch: 6800, accuracy:  0.847, loss:  0.488, dLoss: 0.43285734844641655, rLoss: 0.05543849863243413, lr: 0.0498306008723345\n",
      "epoch: 6900, accuracy:  0.849, loss:  0.493, dLoss: 0.43913554523287984, rLoss: 0.054204312697217844, lr: 0.04982811790727884\n",
      "epoch: 7000, accuracy:  0.835, loss:  0.505, dLoss: 0.4431116900651145, rLoss: 0.06179596508919971, lr: 0.04982563518965381\n",
      "epoch: 7100, accuracy:  0.843, loss:  0.491, dLoss: 0.4349344359641411, rLoss: 0.05626005184260553, lr: 0.049823152719422406\n",
      "epoch: 7200, accuracy:  0.821, loss:  0.491, dLoss: 0.42896526019769954, rLoss: 0.06194975626972701, lr: 0.049820670496547675\n",
      "epoch: 7300, accuracy:  0.843, loss:  0.497, dLoss: 0.4411491285161541, rLoss: 0.05591731818714891, lr: 0.04981818852099264\n",
      "epoch: 7400, accuracy:  0.695, loss:  0.868, dLoss: 0.8135823308449354, rLoss: 0.05464190343834327, lr: 0.049815706792720335\n",
      "epoch: 7500, accuracy:  0.841, loss:  0.493, dLoss: 0.4344030992051087, rLoss: 0.05854422415409204, lr: 0.0498132253116938\n",
      "epoch: 7600, accuracy:  0.836, loss:  0.482, dLoss: 0.4275792585828699, rLoss: 0.054755552933572885, lr: 0.04981074407787611\n",
      "epoch: 7700, accuracy:  0.853, loss:  0.480, dLoss: 0.4269152727248711, rLoss: 0.0527698206874029, lr: 0.049808263091230306\n",
      "epoch: 7800, accuracy:  0.844, loss:  0.486, dLoss: 0.4349178382566203, rLoss: 0.0514513986554251, lr: 0.04980578235171948\n",
      "epoch: 7900, accuracy:  0.846, loss:  0.477, dLoss: 0.42590527565404895, rLoss: 0.051551631784716556, lr: 0.04980330185930667\n",
      "epoch: 8000, accuracy:  0.834, loss:  0.465, dLoss: 0.4151951266593312, rLoss: 0.04998054796913987, lr: 0.04980082161395499\n",
      "epoch: 8100, accuracy:  0.851, loss:  0.484, dLoss: 0.4346497173722899, rLoss: 0.04965689409040886, lr: 0.04979834161562752\n",
      "epoch: 8200, accuracy:  0.831, loss:  0.524, dLoss: 0.4542808407937639, rLoss: 0.0693014831072851, lr: 0.04979586186428736\n",
      "epoch: 8300, accuracy:  0.829, loss:  0.518, dLoss: 0.4448245445412935, rLoss: 0.0732737224939515, lr: 0.04979338235989761\n",
      "epoch: 8400, accuracy:  0.840, loss:  0.507, dLoss: 0.44307617961923884, rLoss: 0.06432319603002962, lr: 0.04979090310242139\n",
      "epoch: 8500, accuracy:  0.812, loss:  0.531, dLoss: 0.4699860957088642, rLoss: 0.0607498749839711, lr: 0.049788424091821805\n",
      "epoch: 8600, accuracy:  0.836, loss:  0.509, dLoss: 0.4507888426033177, rLoss: 0.05790412877244652, lr: 0.049785945328062006\n",
      "epoch: 8700, accuracy:  0.837, loss:  0.505, dLoss: 0.44506222840724063, rLoss: 0.060363480848132506, lr: 0.0497834668111051\n",
      "epoch: 8800, accuracy:  0.825, loss:  0.502, dLoss: 0.4445281821162734, rLoss: 0.05719339423301433, lr: 0.049780988540914256\n",
      "epoch: 8900, accuracy:  0.819, loss:  0.512, dLoss: 0.4532855963410781, rLoss: 0.05884263678975033, lr: 0.0497785105174526\n",
      "epoch: 9000, accuracy:  0.819, loss:  0.587, dLoss: 0.5172864184493933, rLoss: 0.07015908605924003, lr: 0.04977603274068329\n",
      "epoch: 9100, accuracy:  0.764, loss:  0.633, dLoss: 0.5550909232757352, rLoss: 0.07828746651567413, lr: 0.04977355521056952\n",
      "epoch: 9200, accuracy:  0.835, loss:  0.531, dLoss: 0.45500697938067514, rLoss: 0.07643637741669379, lr: 0.049771077927074414\n",
      "epoch: 9300, accuracy:  0.845, loss:  0.499, dLoss: 0.42982715785284364, rLoss: 0.06920580328033259, lr: 0.0497686008901612\n",
      "epoch: 9400, accuracy:  0.823, loss:  0.527, dLoss: 0.4574148303438672, rLoss: 0.06921912823127187, lr: 0.04976612409979302\n",
      "epoch: 9500, accuracy:  0.836, loss:  0.505, dLoss: 0.43823457004019756, rLoss: 0.06630382983386228, lr: 0.0497636475559331\n",
      "epoch: 9600, accuracy:  0.845, loss:  0.490, dLoss: 0.42990184587556934, rLoss: 0.059959737216964196, lr: 0.049761171258544616\n",
      "epoch: 9700, accuracy:  0.838, loss:  0.529, dLoss: 0.4710877774531662, rLoss: 0.05839921289329084, lr: 0.0497586952075908\n",
      "epoch: 9800, accuracy:  0.835, loss:  0.505, dLoss: 0.44853143465479495, rLoss: 0.056124828012147354, lr: 0.04975621940303483\n",
      "epoch: 9900, accuracy:  0.845, loss:  0.467, dLoss: 0.41347172803299737, rLoss: 0.054016843291591794, lr: 0.049753743844839965\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's fine performance, but we should run our validation set as well, and see how the model performs!",
   "id": "b8a99d37b83e267"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:18:01.672961Z",
     "start_time": "2025-06-22T18:18:01.640621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model validation\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = activationLoss.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(activationLoss.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f\"validation: accuracy: {accuracy: .3f}, loss: {loss: .3f}\")"
   ],
   "id": "721bf1f59458b005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: accuracy:  0.870, loss:  0.358\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So! Now we actually have something where the model performs better on a validation set than on the training set. That's interesting, and means we actually are doing a pretty good job!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukieła!"
   ],
   "id": "20277052be70c509"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
