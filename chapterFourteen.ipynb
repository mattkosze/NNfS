{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 14: L1 & L2 Regularization",
   "id": "52b755f2ef209c29"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-21T23:28:31.905933Z",
     "start_time": "2025-06-21T23:28:31.470152Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from timeit import timeit\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy, SoftMaxCategoricalCrossEntropy, SGD, AdaGrad, RMSProp, Adam"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regularization methods are those which reduce generalization error. The first ones that we'll use here are L1 and L2 regularization. They are used to calculate to calculate a penalty number added to the loss value to penalize the model for large weights and biases. Large weights may indicate that a neuron is attempting to memorize a data element, so it makes sense that we would try to curtail them.  \n",
    "\n",
    "## Section 1: The Forward Pass\n",
    "\n",
    "L1 regularization's penalty is the sum of all the absolute values for the weights and biases. This is a linear penalty as regularization loss return by this function is directly proportional to parameter values. L2 regularization's penalty is, on the other hand, the sum of the squared values of the weights and biases. L2's non-linear approach penalizes larger weights and biases more than smaller ones because of the square function. \n",
    "\n",
    "L2 reg. is more commonly used because it doesn't affect small parameter values much while also preventing the model's larger weights from growing disproportionally. In contrast, L1 reg. (due to its linear nature) penalizes small weights more the large ones -- which is why it may be combined with L2 reg. but generally infrequently used overall. To make this work, we use a hyperparameter lambda, where the higher the lambda the higher the penalty applied is.\n",
    "\n",
    "The equation of L1 reg. of weights is:\n",
    "$$\n",
    "L_{1w} = \\lambda \\sum_{m} |W_{m}|\n",
    "$$   \n",
    "The equation of L1 reg. of biases is:\n",
    "$$\n",
    "L_{1b} = \\lambda \\sum_{n} |b_{n}|\n",
    "$$\n",
    "\n",
    "The equation of L2 reg. of weights is:\n",
    "$$\n",
    "L_{2w} = \\lambda \\sum_{m} |w^{2}_{m}|\n",
    "$$\n",
    "The equation of L2 reg. of biases is:\n",
    "$$\n",
    "L_{2b} = \\lambda \\sum_{n} |b^{2}_{n}|\n",
    "$$\n",
    "\n",
    "The overall loss then becomes:\n",
    "$$\n",
    "Loss = DataLoss + L_{1w} + L_{1b} + L_{2w} + L_{2b}\n",
    "$$\n",
    "\n",
    "We can do this in code with the following:\n",
    "```\n",
    "l1w = lambda_l1w * sum(abs(weights))\n",
    "l1b = lambda_l1b * sum(abs(biases))\n",
    "l2w = lambda_l2w * sum(abs(weights ** 2))\n",
    "l2b = lambda_l2b * sum(abs(biases ** 2))\n",
    "loss = dataLoss + l1w + l1b + l2w + l2b\n",
    "```\n",
    "\n",
    "That's pretty straightforward, so we can now implement that in the dense layer class. I'll show the modifications below in an example class, but I'll make them all directly in the classes.py."
   ],
   "id": "214ca83a0b6169ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ExampleDense:\n",
    "    # MODIFIED: added the weight and bias inputs + storage\n",
    "    def __init__(self, nInputs, nNeurons, weightl1=0, weightl2=0, biasl1=0, biasl2=0):\n",
    "        ...\n",
    "        # Store regularization strength\n",
    "        self.weightl1 = weightl1\n",
    "        self.weightl2 = weightl2\n",
    "        self.biasl1 = biasl1\n",
    "        self.biasl2 = biasl2"
   ],
   "id": "fee40d372cd53b6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we also need to update our loss class to make sure it accounts for these! We'll add this method in our general Loss class, because that is extended among all our later classes, meaning it'll be accessible there too!",
   "id": "710e445862fbd9d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ExampleLoss:\n",
    "    # NEW: method for regularization loss\n",
    "    def regularizationLoss(self, layer):\n",
    "        # Set it to 0 by default\n",
    "        regLoss = 0\n",
    "        \n",
    "        # L1 reg for weights\n",
    "        if layer.weightl1 > 0:\n",
    "            regLoss += layer.weightl1 * np.sum(np.abs(layer.weights))\n",
    "        \n",
    "        # L1 reg for biases\n",
    "        if layer.biasl1 > 0:\n",
    "            regLoss += layer.biasl1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "        # L2 reg for weights\n",
    "        if layer.weightl2 > 0:\n",
    "            regLoss += layer.weightl2 * np.sum(layer.weights ** 2)\n",
    "            \n",
    "        # L2 reg for bises\n",
    "        if layer.biasl2 > 0:\n",
    "            regLoss += layer.biasl2 * np.sum(layer.biases ** 2)\n",
    "            \n",
    "        return regLoss"
   ],
   "id": "26859022e324ad63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, we will incorporate this into our model loss calculation as such:\n",
    "```\n",
    "# Calculate the loss\n",
    "dataLoss = loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Calculate reg. penalty\n",
    "regLoss = loss_function.regularizationLoss(dense1) + loss_function.regularizationLoss(dense2)\n",
    "\n",
    "# Total loss calculation\n",
    "loss = dataLoss + regLoss\n",
    "```\n",
    "\n",
    "We must also do the same for the backward pass!\n",
    "\n",
    "## Section 2: The Backward Pass\n",
    "\n",
    "The derivative of the L2 reg. function is:\n",
    "$$\n",
    "L_{2w} = \\lambda \\sum_{m} w^{2}_{m} \\rightarrow 2\\lambda w_{m}\n",
    "$$\n",
    "\n",
    "I skipped a bunch of the book's calculus there, but I assume most people won't be needing it.\n",
    "\n",
    "The derivative of the L1 reg. function is a little more complicated, but it is:\n",
    "$$\n",
    "L_{1w} = \\lambda \\sum_{m} |w_{m}| \\rightarrow \n",
    "\\begin{array}\n",
    "\\lambda \\text{ if } w_{m} > 0 \\\\\n",
    "-\\lambda \\text{ if } w_{m} < 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can write this in python below:"
   ],
   "id": "9c4553bcc4e2da63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weights = [0.2, 0.8, -0.5]\n",
    "dl1 = []\n",
    "for weight in weights:\n",
    "    if weight >= 0:\n",
    "        dl1.append(1)\n",
    "    else:\n",
    "        dl1.append(-1)\n",
    "print(dl1)"
   ],
   "id": "b67364507468ae5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now multiply this to work with multiple neurons in a layer.",
   "id": "7b6ccc2e90f0ebcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [0.5, -.91, .26, -0.5],\n",
    "           [-.26, -.27, .17, .87]]\n",
    "dl1 = []\n",
    "for neuron in weights:\n",
    "    neuron_dl1 = []\n",
    "    for weight in neuron:\n",
    "        if weight >= 0:\n",
    "            neuron_dl1.append(1)\n",
    "        else:\n",
    "            neuron_dl1.append(-1)\n",
    "    dl1.append(neuron_dl1)\n",
    "print(dl1)"
   ],
   "id": "16aec1e18a2e6b5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can simplify this to just a few lines by leveraging the inbuilt NumPy functionality!",
   "id": "b0856e00ae227c49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -.91, .26, -0.5],\n",
    "                    [-.26, -.27, .17, .87]])\n",
    "dl1 = np.ones_like(weights)\n",
    "\n",
    "dl1[weights < 0] = -1\n",
    "\n",
    "print(dl1)"
   ],
   "id": "2249278ee3b7d165",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With that in mind, lets make some more additions to our DenseLayer class! I'll show the changes below in an example class, and the full changes will be made in the classes.py file.",
   "id": "42b134a28d965679"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ExampleDenseLayer:\n",
    "    # MODIFIED: added l1 and l2 reg functionality\n",
    "    def backward(self, dvalues):\n",
    "        ...\n",
    "        # L1 on weights\n",
    "        if self.weightl1 > 0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights < 0] = -1\n",
    "            self.dweights += self.weightl1 * dl1\n",
    "        # L1 on biases\n",
    "        if self.biasl1 > 0:\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases < 0] = -1\n",
    "            self.dbiases += self.biasl1 * dl1\n",
    "        # L2 on weights\n",
    "        if self.weightl2 > 0:\n",
    "            self.dweights += 2 * self.weightl2 * self.weights\n",
    "        # L2 on biases\n",
    "        if self.biasl2 > 0:\n",
    "            self.dbiases += 2 * self.biasl2 * self.biases\n",
    "\n",
    "        # Gradients on input values; we use weights because it's with respect to the inputs\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ],
   "id": "bcaab36965248f2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Those changes have been reflected in the normal DenseLayer class too! Now, lets run our model using L1 and L2 reg!",
   "id": "dfc98d1e0213e860"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T23:29:22.564692Z",
     "start_time": "2025-06-21T23:28:55.974121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating some training data used the spiral_data function\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "\n",
    "# Create dense layer with 2 input features and 64 output features\n",
    "dense1 = DenseLayer(2, 64, weightl2=5e-4, biasl2=5e-4)\n",
    "\n",
    "# Use a relu activation\n",
    "activation1 = ReLU()\n",
    "\n",
    "# Create a dense layer for our output with 64 as an input and 3 as an output\n",
    "dense2 = DenseLayer(64, 3)\n",
    "\n",
    "# Use a softmax combined with ccel. for our output \n",
    "activationLoss = SoftMaxCategoricalCrossEntropy()\n",
    "\n",
    "# Initialize optimizer as Adagrad with a decay\n",
    "optimizer = Adam(lr=0.05, decay=5e-7)\n",
    "\n",
    "# Create the loop that trains our model in epochs\n",
    "for epoch in range(10000):\n",
    "    # Perform the forward pass, as shown previously\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    # NEW: calculate dataLoss, regLoss, and then add for total loss\n",
    "    dataLoss = activationLoss.forward(dense2.output, y)\n",
    "    regLoss = activationLoss.loss.regularizationLoss(dense1) + activationLoss.loss.regularizationLoss(dense2)\n",
    "    loss = dataLoss + regLoss\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    predictions = np.argmax(activationLoss.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, accuracy: {accuracy: .3f}, loss: {loss: .3f}, dLoss: {dataLoss}, rLoss: {regLoss}, lr: {optimizer.lr_curr}\")\n",
    "        \n",
    "    # Perform the backward pass\n",
    "    activationLoss.backward(activationLoss.output, y)\n",
    "    dense2.backward(activationLoss.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Use the optimizer and update the weights and biases\n",
    "    optimizer.preUpdateParams()\n",
    "    optimizer.updateParams(dense1)\n",
    "    optimizer.updateParams(dense2)\n",
    "    optimizer.postUpdateParams()"
   ],
   "id": "86da9e568c5f8220",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy:  0.333, loss:  1.099, dLoss: 1.098588681034382, rLoss: 6.878172268839509e-06, lr: 0.05\n",
      "epoch: 100, accuracy:  0.681, loss:  0.823, dLoss: 0.7799804883537036, rLoss: 0.04303547960378618, lr: 0.04999752512250644\n",
      "epoch: 200, accuracy:  0.739, loss:  0.698, dLoss: 0.6287478621473813, rLoss: 0.0690696077816451, lr: 0.04999502549496326\n",
      "epoch: 300, accuracy:  0.772, loss:  0.644, dLoss: 0.5647735026479431, rLoss: 0.07935410056235946, lr: 0.049992526117345455\n",
      "epoch: 400, accuracy:  0.776, loss:  0.649, dLoss: 0.567071155027531, rLoss: 0.08237097562283491, lr: 0.04999002698961558\n",
      "epoch: 500, accuracy:  0.800, loss:  0.584, dLoss: 0.5013903111873796, rLoss: 0.08245665009315942, lr: 0.049987528111736124\n",
      "epoch: 600, accuracy:  0.818, loss:  0.552, dLoss: 0.4699856596353743, rLoss: 0.08212274570662345, lr: 0.049985029483669646\n",
      "epoch: 700, accuracy:  0.832, loss:  0.535, dLoss: 0.4541652732545524, rLoss: 0.08118981867032718, lr: 0.049982531105378675\n",
      "epoch: 800, accuracy:  0.844, loss:  0.513, dLoss: 0.4313132713745774, rLoss: 0.08132916860240821, lr: 0.04998003297682575\n",
      "epoch: 900, accuracy:  0.842, loss:  0.511, dLoss: 0.42606693266281437, rLoss: 0.0845602278826272, lr: 0.049977535097973466\n",
      "epoch: 1000, accuracy:  0.846, loss:  0.493, dLoss: 0.4096012753799823, rLoss: 0.08341771768220464, lr: 0.049975037468784345\n",
      "epoch: 1100, accuracy:  0.850, loss:  0.485, dLoss: 0.40283330824389896, rLoss: 0.0818674098832736, lr: 0.049972540089220974\n",
      "epoch: 1200, accuracy:  0.851, loss:  0.477, dLoss: 0.39659042218178264, rLoss: 0.08036652090793364, lr: 0.04997004295924593\n",
      "epoch: 1300, accuracy:  0.855, loss:  0.470, dLoss: 0.3911516294564442, rLoss: 0.07882589593350502, lr: 0.04996754607882181\n",
      "epoch: 1400, accuracy:  0.859, loss:  0.468, dLoss: 0.39083618581662694, rLoss: 0.07727265363580774, lr: 0.049965049447911185\n",
      "epoch: 1500, accuracy:  0.863, loss:  0.465, dLoss: 0.38934978691485594, rLoss: 0.07577321675423479, lr: 0.04996255306647668\n",
      "epoch: 1600, accuracy:  0.856, loss:  0.456, dLoss: 0.381387617729193, rLoss: 0.07447561243182638, lr: 0.049960056934480884\n",
      "epoch: 1700, accuracy:  0.854, loss:  0.484, dLoss: 0.40405156412527077, rLoss: 0.07985259986904801, lr: 0.04995756105188642\n",
      "epoch: 1800, accuracy:  0.856, loss:  0.448, dLoss: 0.3695645333219071, rLoss: 0.07872746240771408, lr: 0.049955065418655915\n",
      "epoch: 1900, accuracy:  0.859, loss:  0.442, dLoss: 0.36442525818196625, rLoss: 0.07756761268752178, lr: 0.04995257003475201\n",
      "epoch: 2000, accuracy:  0.861, loss:  0.436, dLoss: 0.3591134379690424, rLoss: 0.07677002223823767, lr: 0.04995007490013731\n",
      "epoch: 2100, accuracy:  0.865, loss:  0.430, dLoss: 0.35405640108602326, rLoss: 0.07598163825975734, lr: 0.0499475800147745\n",
      "epoch: 2200, accuracy:  0.870, loss:  0.424, dLoss: 0.34854070988042857, rLoss: 0.07526717515601516, lr: 0.0499450853786262\n",
      "epoch: 2300, accuracy:  0.871, loss:  0.419, dLoss: 0.34417240583055453, rLoss: 0.07463387366130483, lr: 0.0499425909916551\n",
      "epoch: 2400, accuracy:  0.879, loss:  0.421, dLoss: 0.34647895865766015, rLoss: 0.07403789019272855, lr: 0.04994009685382384\n",
      "epoch: 2500, accuracy:  0.867, loss:  0.414, dLoss: 0.34049009750884274, rLoss: 0.07324828045677392, lr: 0.04993760296509512\n",
      "epoch: 2600, accuracy:  0.872, loss:  0.406, dLoss: 0.33328520954382573, rLoss: 0.07255166942815766, lr: 0.049935109325431604\n",
      "epoch: 2700, accuracy:  0.864, loss:  0.421, dLoss: 0.34893147265894064, rLoss: 0.07235900408685407, lr: 0.049932615934796004\n",
      "epoch: 2800, accuracy:  0.875, loss:  0.398, dLoss: 0.3256942198420299, rLoss: 0.07190465139070319, lr: 0.04993012279315098\n",
      "epoch: 2900, accuracy:  0.873, loss:  0.397, dLoss: 0.32587742883355014, rLoss: 0.07108814053526662, lr: 0.049927629900459285\n",
      "epoch: 3000, accuracy:  0.877, loss:  0.394, dLoss: 0.32307142961715984, rLoss: 0.07047602998268736, lr: 0.049925137256683606\n",
      "epoch: 3100, accuracy:  0.874, loss:  0.391, dLoss: 0.3214374371737669, rLoss: 0.06968421270663556, lr: 0.04992264486178666\n",
      "epoch: 3200, accuracy:  0.867, loss:  0.416, dLoss: 0.33685847470746544, rLoss: 0.07960509415732402, lr: 0.04992015271573119\n",
      "epoch: 3300, accuracy:  0.880, loss:  0.394, dLoss: 0.3162581651778654, rLoss: 0.0777079265120752, lr: 0.04991766081847992\n",
      "epoch: 3400, accuracy:  0.879, loss:  0.389, dLoss: 0.3126688241823838, rLoss: 0.07621483487431244, lr: 0.049915169169995596\n",
      "epoch: 3500, accuracy:  0.880, loss:  0.385, dLoss: 0.31058406583709536, rLoss: 0.07491458486040109, lr: 0.049912677770240964\n",
      "epoch: 3600, accuracy:  0.882, loss:  0.382, dLoss: 0.3086750482324181, rLoss: 0.07378029691972812, lr: 0.049910186619178794\n",
      "epoch: 3700, accuracy:  0.882, loss:  0.380, dLoss: 0.3070373001607198, rLoss: 0.07266150310075019, lr: 0.04990769571677183\n",
      "epoch: 3800, accuracy:  0.883, loss:  0.377, dLoss: 0.3055069017145886, rLoss: 0.07159648062175956, lr: 0.04990520506298287\n",
      "epoch: 3900, accuracy:  0.883, loss:  0.375, dLoss: 0.3040193812113414, rLoss: 0.07057820983569943, lr: 0.04990271465777467\n",
      "epoch: 4000, accuracy:  0.874, loss:  0.375, dLoss: 0.3057115329936993, rLoss: 0.06968182361069242, lr: 0.049900224501110035\n",
      "epoch: 4100, accuracy:  0.880, loss:  0.371, dLoss: 0.3022447407382834, rLoss: 0.06902050019386759, lr: 0.04989773459295174\n",
      "epoch: 4200, accuracy:  0.883, loss:  0.368, dLoss: 0.2998284587905005, rLoss: 0.06826976989848182, lr: 0.04989524493326262\n",
      "epoch: 4300, accuracy:  0.876, loss:  0.369, dLoss: 0.3017554942707149, rLoss: 0.06757316803176086, lr: 0.04989275552200545\n",
      "epoch: 4400, accuracy:  0.886, loss:  0.372, dLoss: 0.30475235507410303, rLoss: 0.06689309519698587, lr: 0.04989026635914307\n",
      "epoch: 4500, accuracy:  0.887, loss:  0.371, dLoss: 0.30443027352520385, rLoss: 0.0662508909628287, lr: 0.04988777744463829\n",
      "epoch: 4600, accuracy:  0.887, loss:  0.367, dLoss: 0.3014400926378768, rLoss: 0.06563041871046252, lr: 0.049885288778453954\n",
      "epoch: 4700, accuracy:  0.887, loss:  0.362, dLoss: 0.2966963906458677, rLoss: 0.06499022853109807, lr: 0.049882800360552884\n",
      "epoch: 4800, accuracy:  0.873, loss:  0.376, dLoss: 0.3118286719793649, rLoss: 0.06443044422677469, lr: 0.04988031219089794\n",
      "epoch: 4900, accuracy:  0.873, loss:  0.372, dLoss: 0.30785501304778773, rLoss: 0.06385083671098839, lr: 0.049877824269451976\n",
      "epoch: 5000, accuracy:  0.875, loss:  0.364, dLoss: 0.3008177319518527, rLoss: 0.06334619661579935, lr: 0.04987533659617785\n",
      "epoch: 5100, accuracy:  0.879, loss:  0.394, dLoss: 0.3207276543748783, rLoss: 0.07293300600413968, lr: 0.04987284917103844\n",
      "epoch: 5200, accuracy:  0.889, loss:  0.360, dLoss: 0.28843461680792754, rLoss: 0.07124089315536185, lr: 0.04987036199399661\n",
      "epoch: 5300, accuracy:  0.890, loss:  0.355, dLoss: 0.28538692416709954, rLoss: 0.069900916430808, lr: 0.04986787506501525\n",
      "epoch: 5400, accuracy:  0.891, loss:  0.353, dLoss: 0.28387581373541854, rLoss: 0.06871045877051393, lr: 0.04986538838405724\n",
      "epoch: 5500, accuracy:  0.891, loss:  0.351, dLoss: 0.28304137583035704, rLoss: 0.06756917810018076, lr: 0.049862901951085496\n",
      "epoch: 5600, accuracy:  0.891, loss:  0.349, dLoss: 0.28203695767190706, rLoss: 0.06656487223401161, lr: 0.049860415766062906\n",
      "epoch: 5700, accuracy:  0.893, loss:  0.347, dLoss: 0.2812701332350877, rLoss: 0.06557504357532892, lr: 0.0498579298289524\n",
      "epoch: 5800, accuracy:  0.894, loss:  0.345, dLoss: 0.2805024593544644, rLoss: 0.06463038309752805, lr: 0.04985544413971689\n",
      "epoch: 5900, accuracy:  0.892, loss:  0.349, dLoss: 0.28494654875017655, rLoss: 0.06375597756110196, lr: 0.049852958698319315\n",
      "epoch: 6000, accuracy:  0.888, loss:  0.351, dLoss: 0.28824817953603105, rLoss: 0.06307409905036428, lr: 0.04985047350472258\n",
      "epoch: 6100, accuracy:  0.895, loss:  0.342, dLoss: 0.2800035481477861, rLoss: 0.06247575396855575, lr: 0.04984798855888967\n",
      "epoch: 6200, accuracy:  0.893, loss:  0.340, dLoss: 0.27792633786181875, rLoss: 0.06191687494482283, lr: 0.049845503860783506\n",
      "epoch: 6300, accuracy:  0.893, loss:  0.342, dLoss: 0.2807824776292442, rLoss: 0.06140511650260319, lr: 0.049843019410367055\n",
      "epoch: 6400, accuracy:  0.893, loss:  0.342, dLoss: 0.2806080524999682, rLoss: 0.060970619711379445, lr: 0.04984053520760327\n",
      "epoch: 6500, accuracy:  0.894, loss:  0.338, dLoss: 0.27712290500192377, rLoss: 0.06050830658605137, lr: 0.049838051252455155\n",
      "epoch: 6600, accuracy:  0.893, loss:  0.340, dLoss: 0.27951786297666315, rLoss: 0.06012389906299541, lr: 0.049835567544885655\n",
      "epoch: 6700, accuracy:  0.895, loss:  0.334, dLoss: 0.27464290111059664, rLoss: 0.059711623121797654, lr: 0.04983308408485778\n",
      "epoch: 6800, accuracy:  0.883, loss:  0.343, dLoss: 0.2836974646256624, rLoss: 0.05930383400566967, lr: 0.0498306008723345\n",
      "epoch: 6900, accuracy:  0.884, loss:  0.348, dLoss: 0.28903506987963146, rLoss: 0.05897744214179401, lr: 0.04982811790727884\n",
      "epoch: 7000, accuracy:  0.867, loss:  0.428, dLoss: 0.3592154137438785, rLoss: 0.0684728480414063, lr: 0.04982563518965381\n",
      "epoch: 7100, accuracy:  0.889, loss:  0.369, dLoss: 0.3019242140417433, rLoss: 0.06740629210803617, lr: 0.049823152719422406\n",
      "epoch: 7200, accuracy:  0.891, loss:  0.362, dLoss: 0.2957341281886868, rLoss: 0.0663563268128916, lr: 0.049820670496547675\n",
      "epoch: 7300, accuracy:  0.893, loss:  0.359, dLoss: 0.29328360144931437, rLoss: 0.06556607111043936, lr: 0.04981818852099264\n",
      "epoch: 7400, accuracy:  0.894, loss:  0.356, dLoss: 0.29148802942640606, rLoss: 0.06483472179186159, lr: 0.049815706792720335\n",
      "epoch: 7500, accuracy:  0.893, loss:  0.354, dLoss: 0.2900457995712165, rLoss: 0.06411156054664326, lr: 0.0498132253116938\n",
      "epoch: 7600, accuracy:  0.893, loss:  0.352, dLoss: 0.28888470996165455, rLoss: 0.06340447241827651, lr: 0.04981074407787611\n",
      "epoch: 7700, accuracy:  0.894, loss:  0.350, dLoss: 0.28776614045138255, rLoss: 0.0627097052286257, lr: 0.049808263091230306\n",
      "epoch: 7800, accuracy:  0.893, loss:  0.349, dLoss: 0.2867092064665867, rLoss: 0.06207885197307998, lr: 0.04980578235171948\n",
      "epoch: 7900, accuracy:  0.887, loss:  0.353, dLoss: 0.29105201429320393, rLoss: 0.061490562145661884, lr: 0.04980330185930667\n",
      "epoch: 8000, accuracy:  0.887, loss:  0.352, dLoss: 0.2905427481402654, rLoss: 0.06095833699169283, lr: 0.04980082161395499\n",
      "epoch: 8100, accuracy:  0.891, loss:  0.354, dLoss: 0.2937222389270869, rLoss: 0.06043710866774327, lr: 0.04979834161562752\n",
      "epoch: 8200, accuracy:  0.883, loss:  0.352, dLoss: 0.29212961979492397, rLoss: 0.05998311301012818, lr: 0.04979586186428736\n",
      "epoch: 8300, accuracy:  0.891, loss:  0.347, dLoss: 0.2872818536773759, rLoss: 0.059523179501146696, lr: 0.04979338235989761\n",
      "epoch: 8400, accuracy:  0.892, loss:  0.350, dLoss: 0.29120683174018497, rLoss: 0.05908945212859325, lr: 0.04979090310242139\n",
      "epoch: 8500, accuracy:  0.875, loss:  0.393, dLoss: 0.32432050792890543, rLoss: 0.06846774668741058, lr: 0.049788424091821805\n",
      "epoch: 8600, accuracy:  0.891, loss:  0.349, dLoss: 0.2817580126746475, rLoss: 0.06705943416879712, lr: 0.049785945328062006\n",
      "epoch: 8700, accuracy:  0.894, loss:  0.346, dLoss: 0.27968246132375035, rLoss: 0.06624150965603308, lr: 0.0497834668111051\n",
      "epoch: 8800, accuracy:  0.896, loss:  0.344, dLoss: 0.2782442873188532, rLoss: 0.06537654253909254, lr: 0.049780988540914256\n",
      "epoch: 8900, accuracy:  0.896, loss:  0.342, dLoss: 0.2779884154039913, rLoss: 0.06445372216598556, lr: 0.0497785105174526\n",
      "epoch: 9000, accuracy:  0.896, loss:  0.341, dLoss: 0.2768863042994928, rLoss: 0.06363221526485682, lr: 0.04977603274068329\n",
      "epoch: 9100, accuracy:  0.895, loss:  0.339, dLoss: 0.2763021810429233, rLoss: 0.06281141729915563, lr: 0.04977355521056952\n",
      "epoch: 9200, accuracy:  0.893, loss:  0.344, dLoss: 0.28158364173212835, rLoss: 0.06220341396826329, lr: 0.049771077927074414\n",
      "epoch: 9300, accuracy:  0.896, loss:  0.337, dLoss: 0.2757532055923877, rLoss: 0.0615745063417905, lr: 0.0497686008901612\n",
      "epoch: 9400, accuracy:  0.898, loss:  0.335, dLoss: 0.2737777999409872, rLoss: 0.061118143337486046, lr: 0.04976612409979302\n",
      "epoch: 9500, accuracy:  0.896, loss:  0.338, dLoss: 0.2771264630939659, rLoss: 0.060595127251336284, lr: 0.0497636475559331\n",
      "epoch: 9600, accuracy:  0.893, loss:  0.340, dLoss: 0.28029242335314825, rLoss: 0.06005792455672859, lr: 0.049761171258544616\n",
      "epoch: 9700, accuracy:  0.897, loss:  0.329, dLoss: 0.2698643982189131, rLoss: 0.05954348583978759, lr: 0.0497586952075908\n",
      "epoch: 9800, accuracy:  0.893, loss:  0.336, dLoss: 0.27734569168717643, rLoss: 0.059008133734489765, lr: 0.04975621940303483\n",
      "epoch: 9900, accuracy:  0.884, loss:  0.349, dLoss: 0.29041974399796094, rLoss: 0.058423656653080386, lr: 0.049753743844839965\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We should also go through a validation run -- which is basically just a forward pass through the model.  ",
   "id": "d833184103b45a41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T23:38:40.683160Z",
     "start_time": "2025-06-21T23:38:40.671620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model validation\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = activationLoss.forward(dense2.output, y_test)\n",
    "\n",
    "predictions = np.argmax(activationLoss.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f\"validation: accuracy: {accuracy: .3f}, loss: {loss: .3f}\")"
   ],
   "id": "635ca81479fc6dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: accuracy:  0.877, loss:  0.316\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That's actually pretty good! Now, what we could do it work on playing with the model or dataset size to see how it impacts the model, but what's next to talk about is dropout!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça!"
   ],
   "id": "8976cb70976d1ced"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
