{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 4: Activation Functions",
   "id": "c1b6572bf352aaa5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.732148Z",
     "start_time": "2025-06-16T22:50:18.729413Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "import math\n",
    "nnfs.init()\n",
    "from nnfs.datasets import spiral_data\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We use activation functions because, if the activation function is non-linear, it allows for deep neural networks to map non-linear functions. \n",
    "\n",
    "Your average neural network will only really consist of two kinds of activation functions: your hidden layer activation function, and your output activation function. You don't \"need\" to have uniform activation functions across all your hidden layer activation functions, but it's pretty standard practice.  "
   ],
   "id": "99313c2dd4708bc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Different Types of Activation Functions\n",
    "### Section 1.1: Step Activation Function\n",
    "The step activation function most closely mimics how a neuron fires in the brain -- in the sense that it's all or nothing (0 or 1). Basically, if the (weight*input + bias) is >= 0, it will fire and output 1, otherwise, it will output a 0. \n",
    "This was commonly used a long time ago, but is no longer commonplace due to the complexity of datasets.\n",
    "\n",
    "### Section 1.2: Linear Activation Function\n",
    "The linear activation function is just the equation of a line: meaning it is quite literally y=x and where the output is equal to the input. This function is commonly used in the output layer of a regression model.\n",
    "\n",
    "### Section 1.3: Sigmoid Activation Function\n",
    "The problem with the traditional step function is it's not a very helpful representation of progress. It either does or does not fire (0 or 1) so it's difficult to say how the current and/or future steps may impact things. So, we sought to have something a little more informative. Hence the sigmoid activation function.\n",
    "The sigmoid was the original \"specific and granular\" activation function used for neural networks. It has the equation y = 1/(1+e^x). It can produce outputs between 0 (toward -inf) and 1 (toward +inf), centering at 0.5 with an input of 0. The sigmoid function has now commonly been replaced with ReLU (rectified linear units -- which I'll dive into next). \n",
    "\n",
    "### Section 1.4: Rectified Linear Units (ReLU)\n",
    "The ReLU function is just y=x clipped to 0 from the left side of the y axis. That means, it's really just a y = max(0, x). \n",
    "This is the most commonly used activation function - primarily because of it's efficiency and speed. It's much simpler than a Sigmoid function, which is non-linear, but it also preserves a similar non-linearity because of the piecewise definition. "
   ],
   "id": "f7166ab8dad9c2fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 2: Why do we use activation functions? + Why does ReLU work?\n",
    "\n",
    "One of the main reasons why we use activation functions is to model non-linear functions. Without an activation function specified, the activation function is just inherently y=x, meaning it can only model linear relationships. \n",
    "\n",
    "It's certainly unintuitive that you can use a piecewise linear function to model non-linear relationships, but it actually works when you think about it. That's because you can add and combine any amount of ReLU activations to create non-linear combinations. I probably botched that, but it makes sense to me. If you need a better explanation, I'd recommend looking [here](https://blog.dailydoseofds.com/p/a-visual-and-intuitive-guide-to-what).\n",
    "\n"
   ],
   "id": "e7dee6183a9b5a33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Section 3: Coding a ReLU activation function\n",
    "\n",
    "The ReLU activation function can be coded as shown below:"
   ],
   "id": "d52c1879467477a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.780328Z",
     "start_time": "2025-06-16T22:50:18.774036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Our placeholder inputs\n",
    "inputs = [0, 3, -4, -5, 10, 5, 10, 3]\n",
    "\n",
    "# A simple loop iter which assigns 0 to the value if the value is less than 0, else just use the value.\n",
    "outputs = [(max(0,x)) for x in inputs]\n",
    "\n",
    "print(f\"Before ReLU: {inputs}\")\n",
    "print(f\"After ReLU: {outputs}\")\n",
    "\n",
    "# So, let's instantiate a ReLU class! P.S. this is just for an example, I'll also instantiate it in a separate python file so we can reference it everywhere, which will just be called the \"Relu\" class.\n",
    "class ReLUExample:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # We can leverage the np.maximum method, which just carries out the ReLU operation for us on the whole list in one go.  \n",
    "        self.output = np.maximum(0, inputs)"
   ],
   "id": "14eeebad660bb8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: [0, 3, -4, -5, 10, 5, 10, 3]\n",
      "After ReLU: [0, 3, 0, 0, 10, 5, 10, 3]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, let's combine this with our knowledge and implementation of dense layers to create a working layer with a ReLU activation function:",
   "id": "6813a86c6e8bc652"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.790484Z",
     "start_time": "2025-06-16T22:50:18.783795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating non-linear placeholder data as shown in chapter three \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Instantiating our first dense layer and ReLU as it's activation function\n",
    "Layer1 = DenseLayer(2, 3)\n",
    "Activation1 = ReLU()\n",
    "\n",
    "# One forward pass with our training data\n",
    "Layer1.forward(X)\n",
    "Activation1.forward(Layer1.output)\n",
    "\n",
    "# Let's just view the first 5 rows of outputs...\n",
    "print(Activation1.output[:5])"
   ],
   "id": "7f7181381848998d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: The Softmax Activation Function\n",
    "\n",
    "All the activation functions that we've seen before (such as the ReLU, Sigmoid, etc...) have been hidden layer activation functions. The Softmax activation function is the first one that we've seen which is normalized. That means: the summation of all outputs adds up to 1, with each output being similar to a confidence score in the category. As a result, the Softmax is particularly useful as an activation function for the output layer, where the output can be used to classify something.\n",
    "The formula for a Softmax is:\n",
    "$$\n",
    "S(i, j) = \\frac{e^{z(i, j)}}{\\sum_{l=1}^{L} e^{z(i, l)}}\n",
    "$$\n",
    "\n",
    "Let's simplify the above down into a python function:"
   ],
   "id": "9cbc4b007a246be9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.798114Z",
     "start_time": "2025-06-16T22:50:18.793269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's create some random inputs, using with we previously learned about np.random.randn(), with size 3 in the range [0, 1]\n",
    "layer_outputs = np.random.randn(3)\n",
    "\n",
    "# Just making a variable e for easier reference \n",
    "e = math.e\n",
    "\n",
    "# I'm going to create bits and pieces so we don't have one giant terrifying formula...\n",
    "exp_vals = [(float(e ** x)) for x in layer_outputs] # just the values from the layer_outputs, but exponentiated\n",
    "sum_exp = sum(exp_vals)\n",
    "normalized_vals = [(x/sum_exp) for x in exp_vals]\n",
    "\n",
    "print(normalized_vals)\n",
    "print(sum(normalized_vals))"
   ],
   "id": "31019679ee2729f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10431084574758263, 0.10301930069728654, 0.7926698535551309]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great, now that was doing it the hard way; we can do this using Numpy as well to streamline our code, as shown below:",
   "id": "806530e44ce5526c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.804742Z",
     "start_time": "2025-06-16T22:50:18.801429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Again creating a random input list of size 3 in the range of [0, 1]\n",
    "layer_outputs = np.random.randn(3)\n",
    "\n",
    "#exponentiating the values again\n",
    "exp_vals = np.exp(layer_outputs)\n",
    "#normalizing the values\n",
    "normalized_vals = exp_vals / np.sum(exp_vals) \n",
    "\n",
    "print(normalized_vals)\n",
    "print(sum(normalized_vals))"
   ],
   "id": "9fa168c8bb56fcaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18272969 0.18751894 0.6297514 ]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next thing to do here is to modify our softmax function so that it accepts layer outputs in batches, which will be done below: ",
   "id": "605194e03fd93351"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.809375Z",
     "start_time": "2025-06-16T22:50:18.806325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_outputs = np.random.randn(3, 3)\n",
    "\n",
    "exp_vals = np.exp(layer_outputs)\n",
    "probabilities = exp_vals/np.sum(exp_vals, axis=1, keepdims=True)\n",
    "\n",
    "print(probabilities)"
   ],
   "id": "54b3c38e930c22c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21804984 0.32404155 0.45790863]\n",
      " [0.391328   0.34138915 0.2672828 ]\n",
      " [0.40432692 0.38627318 0.20939988]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, I'm sure you're asking, just as I was about 20 minutes before I wrote this, what does the \"axis\" and \"keepdims\" argument mean? I'll break the two down.\n",
    "\n",
    "The axis argument tells np.sum on which dimension to sum. The examples I'll run through are \"axis=None,\" \"axis=0,\" and \"axis=1.\" For each example, before I show you using python what that means, I'll (try to) explain it in English. \n",
    "\n",
    "The \"axis=None\" argument tells np.sum to sum all elements. So it'll go across the whole list, which we can easily visualize using the sum function. \n",
    "To build some intuition here, let's take a 1 x 3 array called Array. Let's say Array = [[a], [b], [c]], then doing np.sum(Array, axis=None) will just return (a+b+c)."
   ],
   "id": "f37872e297ab55eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.829988Z",
     "start_time": "2025-06-16T22:50:18.826499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_outputs = np.random.randn(3, 3)\n",
    "\n",
    "print(f\"Sum of all elements with a normal sum {sum(sum(layer_outputs))}\")\n",
    "print(f\"Sum with an np.sum: {np.sum(layer_outputs, axis=None)}\")"
   ],
   "id": "cfd612f7f178d13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all elements with a normal sum -0.5614225268363953\n",
      "Sum with an np.sum: -0.5614225268363953\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e4db9e8c512eede9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, let's take a look at the \"axis=0\" argument, which will sum values across the rows. We can visualize this using an example 2 x 2 array called Array. Let Array = [[a, b], [c, d]], then doing np.sum(Array, axis=0) will return [a+c, b+d]. ",
   "id": "9f55b2bca33bba65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.836241Z",
     "start_time": "2025-06-16T22:50:18.831956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_outputs = np.random.randn(2, 2)\n",
    "\n",
    "print(f\"Plain layer_outputs: \\n{layer_outputs}\")\n",
    "print(f\"Sum of the columns with an np.sum: {np.sum(layer_outputs, axis=0)}\")"
   ],
   "id": "784afc97b292c448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain layer_outputs: \n",
      "[[ 0.7471883  -1.1889449 ]\n",
      " [ 0.77325296 -1.1838807 ]]\n",
      "Sum of the columns with an np.sum: [ 1.5204413 -2.3728256]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lastly, we can take a look at the \"axis=1\" argument, which will sum values across the columns. Let's again build some intuition here, using an example of a 2 x 2 array called Array. Let's say that Array = [[a, b], [c, d]], then doing np.sum(Array, axis=1) will return [a+b, c+d]. ",
   "id": "16eae534c969b94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.841873Z",
     "start_time": "2025-06-16T22:50:18.838597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_outputs = np.random.randn(2, 2)\n",
    "\n",
    "print(f\"Plain layer_outputs: \\n{layer_outputs}\")\n",
    "print(f\"Sum of all row elements: {[float(sum(layer_outputs[x])) for x in range(len(layer_outputs))]}\")\n",
    "print(f\"Sum of the rows with an np.sum: {np.sum(layer_outputs, axis=1)}\")"
   ],
   "id": "a6940ebfc5dcba36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain layer_outputs: \n",
      "[[-2.6591723   0.60631955]\n",
      " [-1.7558906   0.45093447]]\n",
      "Sum of all row elements: [-2.0528526306152344, -1.3049561977386475]\n",
      "Sum of the rows with an np.sum: [-2.0528526 -1.3049562]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The last thing to talk about here is the \"keepdims=True\" argument, which basically just requires that the output retains the reduced dimensions with shape 1. I'll provide two examples below to make it entirely clear, just in case. ",
   "id": "ac9aaa613ac5d65b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.849544Z",
     "start_time": "2025-06-16T22:50:18.846269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_outputs = np.random.randn(2, 2)\n",
    "\n",
    "# keepdims=False (implied)\n",
    "print(f\"Sum of the rows with keepdims=False: \\n{np.sum(layer_outputs, axis=1, keepdims=False)}\")\n",
    "print(f\"Sum of the rows with keepdims=True: \\n{np.sum(layer_outputs, axis=1, keepdims=True)}\")"
   ],
   "id": "bef2b605f86bc81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the rows with keepdims=False: \n",
      "[0.97553986 0.6151235 ]\n",
      "Sum of the rows with keepdims=True: \n",
      "[[0.97553986]\n",
      " [0.6151235 ]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With all this knowledge, now we can create a Softmax class. I'll write one out here as an example, but there'll also be one imported from classes.py.",
   "id": "f76e78709ff77a47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.855591Z",
     "start_time": "2025-06-16T22:50:18.852639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SoftMaxExample:\n",
    "    def forward(self, inputs):\n",
    "        exp_vals = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # I'll explain why we subtract the largest entry in just a second.\n",
    "        probabilities = exp_vals/np.sum(exp_vals, axis=1, keepdims=True) # We've shown this before!\n",
    "        self.output = probabilities # Just setting the output..."
   ],
   "id": "99327939f8b10e7a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I have a little bit of explaining to do here. The book states there are two main problems that neural networks face: dead neurons and exploding gradients. We know that the first one does, but the exploding gradients problem happens when we have numbers too big and the computer experiences overflow (if you don't know what that is, look it up). \n",
    "\n",
    "I'll show you a little example here, and we'll have to think back to our limit days of calculus. If you think about what the limit of a funtion y=e^x as x approaches -inf, then you'll realize that the limit is y=0. That is because what's really happening is it's becoming y=1/e^inf, where e^inf is just becoming inf and therefore 1/inf is just 0. On the other hand, doing np.exp(0) just returns 1 because (anything)^0 is just 1. We can take advantage of these properties to prevent the computer from overflowing! We just do so by subtracting the largest number from every entry, which will result in our entries being bounded by -inf on the left and 0 on the right, and therefore making all of our np.exp(x) result in an 0.0>=x<=1.0; best of all, it has no impact on our probability distribution. I hope that makes sense -- took me a bit to understand as well. This also has no \n",
    "\n",
    "So now, let's see our SoftMax class in action!"
   ],
   "id": "db6f16fe065c538b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.889802Z",
     "start_time": "2025-06-16T22:50:18.886537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax = SoftMax()\n",
    "softmax.forward([[1,2,3]])\n",
    "print(f\"The probabilities are: {softmax.output}\")"
   ],
   "id": "70454509a3cd9786",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabilities are: [[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's build up on all of that to create a (almost) fully functioning neural network!",
   "id": "7ce3797fae9a4a1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T22:50:18.911852Z",
     "start_time": "2025-06-16T22:50:18.902528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "#Let's initialize our neural net, with layers 1 and 2 being dense layers, but adding a ReLU on the outputs of L1 and a SoftMax on our output layer, L2. \n",
    "Layer1 = DenseLayer(2, 3)\n",
    "Layer2 = DenseLayer(3, 3)\n",
    "Activation1 = ReLU()\n",
    "Activation2 = SoftMax()\n",
    "\n",
    "Layer1.forward(X)\n",
    "Activation1.forward(Layer1.output)\n",
    "Layer2.forward(Activation1.output)\n",
    "Activation2.forward(Layer2.output)\n",
    "\n",
    "print(Activation2.output[:5])"
   ],
   "id": "66b6135bbdc66594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333248 0.33333376 0.33333376]\n",
      " [0.33333203 0.3333339  0.33333406]\n",
      " [0.33333176 0.33333403 0.3333342 ]\n",
      " [0.3333314  0.33333418 0.3333344 ]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is working well, with a near 33% distribution across the classes. That comes as result of the random initialization of weights with np.random.randn, all of which is centered and normally distributed. If this were an actual model, we could then apply an argmax which would then just return the index of the most likely classification.\n",
    "\n",
    "So clearly, we now have an almost functioning model - but it's still totally random. So, what we need to do now is learn about loss functions that can quantify to our model how wrong it is, so it can change its weights to become accurate!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukieła!"
   ],
   "id": "f5b27528e1e58944"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
