{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 16: Binary Logistic Regression",
   "id": "c94c36eb0a22d7cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T01:56:30.976Z",
     "start_time": "2025-06-25T01:56:30.542111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from timeit import timeit\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy, SoftMaxCategoricalCrossEntropy, SGD, AdaGrad, RMSProp, Adam, DropoutLayer, Sigmoid, BinaryCrossEntropyLoss\n",
    "import random"
   ],
   "id": "4964332998a0cb6e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, let's talk about another kind of output layer: the binary logistic regression. So far, we've only talked about the categorical classification -- which always assumes one of the classes is the correct answer as all probabilities must sum to 1. \n",
    "\n",
    "On the other hand, binary logistic regression is a type of output where each output neuron separately represents two classes: 0 in one class and 1 in the other. That means you can have a neuron that represents \"cat vs dog\" or \"cat vs not cat\" or any of the sort. And, you can have many of these output neurons. As a quick example, you could have a neuron that decides \"car vs not car\" and another that decides \"indoors vs outdoors\". \n",
    "\n",
    "Binary logistic regression is a regression type of algorithm, which will differ as we now use a sigmoid activation function for the output instead of our softmax, and binary cross entropy rather than categorical cross entropy.\n",
    "\n",
    "# Section 1: The Sigmoid Activation Function\n",
    "\n",
    "The sigmoid activation function is used with regressors because it scales all inputs (in the range of -inf to +inf) to be bounded between 0 and 1. These bounds represent the two possible classes:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "For simplicity, we often rewrite this as:\n",
    "$$\n",
    "\\sigma(z_{i,j}) = \\frac{1}{1+e^{-z_{i,j}}}\n",
    "$$ \n",
    "\n",
    "This function averages at 0.5 and squishes down to a flat line as it approaches both -inf and inf exponentially fast.\n",
    "\n",
    "I'll skip over all the calculus and just say directly that the derivative is:\n",
    "$$\n",
    "\\frac{d}{dz_{i,j}} = \\sigma_{i,j} \\cdot (1 - \\sigma_{i,j})\n",
    "$$\n",
    "\n",
    "We'll create a python class for this:"
   ],
   "id": "9a3d7422a2e2168a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExampleSigmoid:\n",
    "    def __init__(self, inputs):\n",
    "        # Save input and calculate output\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate the derivative\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ],
   "id": "c220f827d18b3b6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great, but now we need a loss function!\n",
    "\n",
    "## Section 2: Binary Cross-Entropy Loss\n",
    "\n",
    "Binary cross entropy loss is not all that different from categorical cross entropy loss. Instead of only calculating the -log on the target class, we'll sum the log-likelihoods of the correct and incorrect classes.\n",
    "$$\n",
    "L_{i,j} = -y_{i,j} \\cdot log(\\hat{y}_{i,j}) - (1 - y_{i,j}) \\cdot log(1-\\hat{y}_{i,j})\n",
    "$$ \n",
    "\n",
    "However, we must remember that a model can contain multiple binary outputs, then loss calculated on a single output is going to be a vector of losses containing one value for each output. Therefore, we need to calculate a mean from these losses:\n",
    "$$\n",
    "L_{i} = \\frac{1}{J} \\sum_{j} L_{i,j}\n",
    "$$\n",
    "Where i is the current sample, index j means the current output of sample i, and J means the total number of outputs. We can then perform this whole operation using numpy, where the sample_losses are a collection of various $L_{i}$\n",
    "```\n",
    "sample_losses = np.mean(sample_losses, axis=-1)\n",
    "```\n",
    "That is, where the \"axis=-1\" parameter means to calculate the mean along the last dimension.\n",
    "\n",
    "## Section 3: Binary Cross-Entropy Loss Derivative\n",
    "\n",
    "Here is the fun math-y part I'm sure we've all been waiting for! Let's break this down in two steps, with the equation for $L_{i}$ first.\n",
    "\n",
    "We're first calculate the partial derivative of the loss function with respect to the predicted input:\n",
    "$$\n",
    "\\frac{\\partial L_{i,j}}{\\partial \\hat{y}_{i,j}} = \\frac{\\partial}{\\partial \\hat{y}_{i,j}}\n",
    "[-y_{i,j} \\cdot log(\\hat{y}_{i,j}) - (1 - y_{i,j}) \\cdot log(1-\\hat{y}_{i,j})]\n",
    "$$ \n",
    "After a bunch of calculus, that simplifies down to:\n",
    "$$\n",
    "-\\frac{y_{i,j}}{\\hat{y}_{i,j}} + \\frac{1-y_{i,j}}{1-\\hat{y}_{i,j}}\n",
    "$$\n",
    "And then ultimately down to:\n",
    "$$\n",
    "-(\\frac{y_{i,j}}{\\hat{y}_{i,j}} - \\frac{1-y_{i,j}}{1-\\hat{y}_{i,j}})\n",
    "$$\n",
    "\n",
    "Now, we need to find the partial derivative of the sample loss with respect to each input!\n",
    "$$\n",
    "\\frac{\\partial L_{i}}{\\partial \\hat{y}_{i,j}} = \\frac{\\partial}{\\partial L_{i,j}} [\\frac{1}{J} \\sum_{j} L_{i,j}]\n",
    "$$    \n",
    "Which ultimately simplifies down to:\n",
    "$$\n",
    "\\frac{1}{J}\n",
    "$$\n",
    "\n",
    "Therefore, our equation for the partial derivative of a sample loss with respect to a single output loss becomes:\n",
    "$$\n",
    "-\\frac{1}{J} \\cdot (\\frac{y_{i,j}}{\\hat{y}_{i,j}} - \\frac{1-y_{i,j}}{1-\\hat{y}_{i,j}})\n",
    "$$\n",
    "\n",
    "The only thing we need to do now is implement our code for this, so let's do that now. As usual, we'll create an example class here with the full one implemented in our classes.py."
   ],
   "id": "ce76d242e81be9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExampleBinaryCrossEntropyLoss(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data on both sides to prevent division by 0, both sides to prevent skewing data\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        # Calculate sample-wise losses\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Sample size\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs per sample\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Clip data\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs /= samples"
   ],
   "id": "2519d256b8f1842f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With that done, we can now spin up a model with this new binary cross entropy loss class! ",
   "id": "9c7469806a0629a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T02:20:03.748628Z",
     "start_time": "2025-06-25T02:20:01.204034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating some training data used the spiral_data function\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Need to reshape our data as it is no longer sparse\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create dense layer with 2 input features and 64 output features\n",
    "# Changed the layer size from 64 to 512 to improve accuracy\n",
    "dense1 = DenseLayer(2, 64, weightl2=5e-4, biasl2=5e-4)\n",
    "\n",
    "# Use a relu activation\n",
    "activation1 = ReLU()\n",
    "\n",
    "# Create a dense layer for our output with 64 as an input and 3 as an output\n",
    "# NEW: changed the layer size from 64 to 512 to improve accuracy\n",
    "dense2 = DenseLayer(64, 1)\n",
    "\n",
    "# Use a softmax combined with ccel. for our output \n",
    "activation2 = Sigmoid()\n",
    "\n",
    "# Set up the loss function\n",
    "loss_function = BinaryCrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer as Adam with a decay\n",
    "optimizer = Adam(decay=5e-7)\n",
    "\n",
    "# Create the loop that trains our model in epochs\n",
    "for epoch in range(10000):\n",
    "    # Perform the forward pass, as shown previously\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    # calculate dataLoss, regLoss, and then add for total loss\n",
    "    dataLoss = loss_function.calculate(activation2.output, y)\n",
    "    regLoss = loss_function.regularizationLoss(dense1) + loss_function.regularizationLoss(dense2)\n",
    "    loss = dataLoss + regLoss\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, accuracy: {accuracy: .3f}, loss: {loss: .3f}, dLoss: {dataLoss}, rLoss: {regLoss}, lr: {optimizer.lr_curr}\")\n",
    "        \n",
    "    # Perform the backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Use the optimizer and update the weights and biases\n",
    "    optimizer.preUpdateParams()\n",
    "    optimizer.updateParams(dense1)\n",
    "    optimizer.updateParams(dense2)\n",
    "    optimizer.postUpdateParams()"
   ],
   "id": "b196de923a68578f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy:  0.455, loss:  0.693, dLoss: 0.6931611483003809, rLoss: 5.688446779362844e-06, lr: 0.001\n",
      "epoch: 100, accuracy:  0.620, loss:  0.673, dLoss: 0.6722421747805468, rLoss: 0.0008714248605187014, lr: 0.0009999505024501287\n",
      "epoch: 200, accuracy:  0.625, loss:  0.670, dLoss: 0.6685268864345999, rLoss: 0.001215202928005334, lr: 0.0009999005098992651\n",
      "epoch: 300, accuracy:  0.625, loss:  0.667, dLoss: 0.6650754018615368, rLoss: 0.0015082201361740796, lr: 0.000999850522346909\n",
      "epoch: 400, accuracy:  0.625, loss:  0.663, dLoss: 0.6609270207839064, rLoss: 0.0019611363776167105, lr: 0.0009998005397923115\n",
      "epoch: 500, accuracy:  0.615, loss:  0.659, dLoss: 0.6561376435166179, rLoss: 0.0025638725897037967, lr: 0.0009997505622347225\n",
      "epoch: 600, accuracy:  0.615, loss:  0.651, dLoss: 0.6476996340254764, rLoss: 0.0037124576740371067, lr: 0.0009997005896733929\n",
      "epoch: 700, accuracy:  0.620, loss:  0.640, dLoss: 0.6341509154382148, rLoss: 0.0059798251136191916, lr: 0.0009996506221075735\n",
      "epoch: 800, accuracy:  0.700, loss:  0.626, dLoss: 0.6164806697208186, rLoss: 0.009332507630687563, lr: 0.000999600659536515\n",
      "epoch: 900, accuracy:  0.725, loss:  0.609, dLoss: 0.5955176210311105, rLoss: 0.013607339212725852, lr: 0.0009995507019594694\n",
      "epoch: 1000, accuracy:  0.740, loss:  0.592, dLoss: 0.5728222940433472, rLoss: 0.018747744143104005, lr: 0.000999500749375687\n",
      "epoch: 1100, accuracy:  0.740, loss:  0.575, dLoss: 0.5511512473928124, rLoss: 0.024073303738280618, lr: 0.0009994508017844195\n",
      "epoch: 1200, accuracy:  0.740, loss:  0.561, dLoss: 0.5317804570979713, rLoss: 0.029083670989844577, lr: 0.0009994008591849186\n",
      "epoch: 1300, accuracy:  0.755, loss:  0.548, dLoss: 0.514919522640572, rLoss: 0.03354285045554631, lr: 0.0009993509215764362\n",
      "epoch: 1400, accuracy:  0.780, loss:  0.538, dLoss: 0.5004946740973563, rLoss: 0.03733254935947556, lr: 0.0009993009889582235\n",
      "epoch: 1500, accuracy:  0.795, loss:  0.529, dLoss: 0.48811467540043146, rLoss: 0.040501649742729726, lr: 0.0009992510613295335\n",
      "epoch: 1600, accuracy:  0.820, loss:  0.520, dLoss: 0.47731244914160115, rLoss: 0.04314930032997125, lr: 0.0009992011386896176\n",
      "epoch: 1700, accuracy:  0.830, loss:  0.513, dLoss: 0.46794878174287774, rLoss: 0.04534924421564232, lr: 0.0009991512210377285\n",
      "epoch: 1800, accuracy:  0.835, loss:  0.507, dLoss: 0.459991972620127, rLoss: 0.047033310373340714, lr: 0.0009991013083731183\n",
      "epoch: 1900, accuracy:  0.835, loss:  0.501, dLoss: 0.45320437788269524, rLoss: 0.04827197438243384, lr: 0.0009990514006950402\n",
      "epoch: 2000, accuracy:  0.845, loss:  0.495, dLoss: 0.44625784741741376, rLoss: 0.04901624352612958, lr: 0.0009990014980027463\n",
      "epoch: 2100, accuracy:  0.830, loss:  0.488, dLoss: 0.4382956950238998, rLoss: 0.04981292077700541, lr: 0.0009989516002954898\n",
      "epoch: 2200, accuracy:  0.825, loss:  0.483, dLoss: 0.43217786600112673, rLoss: 0.050519327214399556, lr: 0.000998901707572524\n",
      "epoch: 2300, accuracy:  0.825, loss:  0.478, dLoss: 0.4267575202141491, rLoss: 0.05111347259878879, lr: 0.0009988518198331018\n",
      "epoch: 2400, accuracy:  0.820, loss:  0.474, dLoss: 0.4219583251430609, rLoss: 0.05161609683402429, lr: 0.0009988019370764769\n",
      "epoch: 2500, accuracy:  0.820, loss:  0.469, dLoss: 0.4174615163154425, rLoss: 0.051955488070249606, lr: 0.0009987520593019025\n",
      "epoch: 2600, accuracy:  0.820, loss:  0.466, dLoss: 0.4136986026048974, rLoss: 0.052145082821872295, lr: 0.000998702186508632\n",
      "epoch: 2700, accuracy:  0.820, loss:  0.463, dLoss: 0.41031899136952404, rLoss: 0.05223096397511128, lr: 0.00099865231869592\n",
      "epoch: 2800, accuracy:  0.815, loss:  0.459, dLoss: 0.40726905625094917, rLoss: 0.052230045322042964, lr: 0.0009986024558630198\n",
      "epoch: 2900, accuracy:  0.820, loss:  0.457, dLoss: 0.4044631822838563, rLoss: 0.05216630435852964, lr: 0.0009985525980091856\n",
      "epoch: 3000, accuracy:  0.825, loss:  0.454, dLoss: 0.4014118190079591, rLoss: 0.05213307821905553, lr: 0.000998502745133672\n",
      "epoch: 3100, accuracy:  0.840, loss:  0.449, dLoss: 0.3964024540572275, rLoss: 0.052149667619924865, lr: 0.0009984528972357331\n",
      "epoch: 3200, accuracy:  0.840, loss:  0.442, dLoss: 0.38938011450265636, rLoss: 0.05214827096171537, lr: 0.0009984030543146237\n",
      "epoch: 3300, accuracy:  0.845, loss:  0.437, dLoss: 0.38491655022273136, rLoss: 0.052240789537944234, lr: 0.0009983532163695982\n",
      "epoch: 3400, accuracy:  0.845, loss:  0.434, dLoss: 0.3816846851443383, rLoss: 0.05239516229195198, lr: 0.000998303383399912\n",
      "epoch: 3500, accuracy:  0.850, loss:  0.431, dLoss: 0.3784733570764264, rLoss: 0.0524513118606787, lr: 0.0009982535554048193\n",
      "epoch: 3600, accuracy:  0.855, loss:  0.428, dLoss: 0.37559375022709546, rLoss: 0.052473455711155775, lr: 0.000998203732383576\n",
      "epoch: 3700, accuracy:  0.860, loss:  0.425, dLoss: 0.3728958145778323, rLoss: 0.05249807812827385, lr: 0.0009981539143354365\n",
      "epoch: 3800, accuracy:  0.865, loss:  0.423, dLoss: 0.3703962724960212, rLoss: 0.05248541844532189, lr: 0.0009981041012596574\n",
      "epoch: 3900, accuracy:  0.865, loss:  0.421, dLoss: 0.3680989298669158, rLoss: 0.05243120034608196, lr: 0.0009980542931554933\n",
      "epoch: 4000, accuracy:  0.865, loss:  0.418, dLoss: 0.36593407233800107, rLoss: 0.05233969694533553, lr: 0.0009980044900222008\n",
      "epoch: 4100, accuracy:  0.870, loss:  0.416, dLoss: 0.3639447437981723, rLoss: 0.052203747671835464, lr: 0.0009979546918590348\n",
      "epoch: 4200, accuracy:  0.875, loss:  0.414, dLoss: 0.36209897006690395, rLoss: 0.052022434128499224, lr: 0.0009979048986652524\n",
      "epoch: 4300, accuracy:  0.875, loss:  0.412, dLoss: 0.3602064783913967, rLoss: 0.05179362860128949, lr: 0.000997855110440109\n",
      "epoch: 4400, accuracy:  0.870, loss:  0.383, dLoss: 0.3305824999970431, rLoss: 0.05232513871379703, lr: 0.0009978053271828614\n",
      "epoch: 4500, accuracy:  0.885, loss:  0.370, dLoss: 0.3165828458393709, rLoss: 0.05314673031146813, lr: 0.0009977555488927658\n",
      "epoch: 4600, accuracy:  0.900, loss:  0.361, dLoss: 0.3064696109561192, rLoss: 0.054208482423741676, lr: 0.000997705775569079\n",
      "epoch: 4700, accuracy:  0.910, loss:  0.353, dLoss: 0.2972972935915314, rLoss: 0.055415215477313026, lr: 0.0009976560072110577\n",
      "epoch: 4800, accuracy:  0.915, loss:  0.346, dLoss: 0.28935149904515994, rLoss: 0.056911164048008306, lr: 0.0009976062438179587\n",
      "epoch: 4900, accuracy:  0.915, loss:  0.341, dLoss: 0.2829048693216611, rLoss: 0.05827060602496156, lr: 0.0009975564853890394\n",
      "epoch: 5000, accuracy:  0.910, loss:  0.337, dLoss: 0.2772486315444511, rLoss: 0.0594434683360882, lr: 0.000997506731923557\n",
      "epoch: 5100, accuracy:  0.915, loss:  0.332, dLoss: 0.2716781888225032, rLoss: 0.060459373609632557, lr: 0.0009974569834207687\n",
      "epoch: 5200, accuracy:  0.920, loss:  0.328, dLoss: 0.26693806176931284, rLoss: 0.061405133754630324, lr: 0.0009974072398799322\n",
      "epoch: 5300, accuracy:  0.920, loss:  0.325, dLoss: 0.26261810691012266, rLoss: 0.06224774232892764, lr: 0.0009973575013003048\n",
      "epoch: 5400, accuracy:  0.920, loss:  0.322, dLoss: 0.25861199382667854, rLoss: 0.06296355791467423, lr: 0.0009973077676811448\n",
      "epoch: 5500, accuracy:  0.925, loss:  0.318, dLoss: 0.2546883307120752, rLoss: 0.06357208360441197, lr: 0.00099725803902171\n",
      "epoch: 5600, accuracy:  0.930, loss:  0.312, dLoss: 0.2481212377322435, rLoss: 0.0641891383472163, lr: 0.0009972083153212581\n",
      "epoch: 5700, accuracy:  0.930, loss:  0.309, dLoss: 0.24429677784819703, rLoss: 0.0647932117933836, lr: 0.000997158596579048\n",
      "epoch: 5800, accuracy:  0.930, loss:  0.306, dLoss: 0.2410995104167285, rLoss: 0.06528277756146499, lr: 0.0009971088827943377\n",
      "epoch: 5900, accuracy:  0.930, loss:  0.304, dLoss: 0.2382085824997436, rLoss: 0.06566091801361454, lr: 0.0009970591739663862\n",
      "epoch: 6000, accuracy:  0.930, loss:  0.301, dLoss: 0.23554656116893483, rLoss: 0.06595036077939634, lr: 0.0009970094700944517\n",
      "epoch: 6100, accuracy:  0.930, loss:  0.299, dLoss: 0.23310013201468677, rLoss: 0.06614041948712385, lr: 0.0009969597711777935\n",
      "epoch: 6200, accuracy:  0.930, loss:  0.297, dLoss: 0.2308400282540816, rLoss: 0.06624263572207358, lr: 0.00099691007721567\n",
      "epoch: 6300, accuracy:  0.930, loss:  0.295, dLoss: 0.22873229200209289, rLoss: 0.0662809231732806, lr: 0.000996860388207341\n",
      "epoch: 6400, accuracy:  0.935, loss:  0.293, dLoss: 0.22675203405523703, rLoss: 0.06627314035542765, lr: 0.0009968107041520655\n",
      "epoch: 6500, accuracy:  0.935, loss:  0.291, dLoss: 0.22491200873118491, rLoss: 0.06622199840559537, lr: 0.000996761025049103\n",
      "epoch: 6600, accuracy:  0.935, loss:  0.289, dLoss: 0.22315966805058218, rLoss: 0.06613551039157263, lr: 0.000996711350897713\n",
      "epoch: 6700, accuracy:  0.935, loss:  0.288, dLoss: 0.22152060599895884, rLoss: 0.06601368487566193, lr: 0.0009966616816971556\n",
      "epoch: 6800, accuracy:  0.935, loss:  0.286, dLoss: 0.21996304799722188, rLoss: 0.06585755298498625, lr: 0.00099661201744669\n",
      "epoch: 6900, accuracy:  0.935, loss:  0.284, dLoss: 0.21848013697254068, rLoss: 0.06567902797577814, lr: 0.0009965623581455767\n",
      "epoch: 7000, accuracy:  0.935, loss:  0.283, dLoss: 0.21707500977917682, rLoss: 0.06547427052101476, lr: 0.000996512703793076\n",
      "epoch: 7100, accuracy:  0.935, loss:  0.281, dLoss: 0.21573112261817823, rLoss: 0.06524749833451311, lr: 0.0009964630543884481\n",
      "epoch: 7200, accuracy:  0.935, loss:  0.279, dLoss: 0.21445689646833416, rLoss: 0.06500092004003648, lr: 0.0009964134099309536\n",
      "epoch: 7300, accuracy:  0.935, loss:  0.278, dLoss: 0.21319021488426265, rLoss: 0.06476815278567431, lr: 0.0009963637704198528\n",
      "epoch: 7400, accuracy:  0.935, loss:  0.276, dLoss: 0.2119712805996008, rLoss: 0.0645233227406652, lr: 0.0009963141358544066\n",
      "epoch: 7500, accuracy:  0.935, loss:  0.275, dLoss: 0.21081187530690862, rLoss: 0.06426404145801046, lr: 0.000996264506233876\n",
      "epoch: 7600, accuracy:  0.935, loss:  0.274, dLoss: 0.20971202978411518, rLoss: 0.06398768629443785, lr: 0.0009962148815575223\n",
      "epoch: 7700, accuracy:  0.935, loss:  0.272, dLoss: 0.20864454804384613, rLoss: 0.0637021854957606, lr: 0.000996165261824606\n",
      "epoch: 7800, accuracy:  0.935, loss:  0.271, dLoss: 0.20762368152106575, rLoss: 0.0633926373726699, lr: 0.0009961156470343895\n",
      "epoch: 7900, accuracy:  0.935, loss:  0.270, dLoss: 0.20661568759149532, rLoss: 0.06310106234658074, lr: 0.0009960660371861334\n",
      "epoch: 8000, accuracy:  0.935, loss:  0.268, dLoss: 0.20564732402064975, rLoss: 0.0628062150802594, lr: 0.0009960164322790998\n",
      "epoch: 8100, accuracy:  0.935, loss:  0.267, dLoss: 0.2046767018477649, rLoss: 0.06250000347617747, lr: 0.0009959668323125503\n",
      "epoch: 8200, accuracy:  0.935, loss:  0.266, dLoss: 0.20376535826960165, rLoss: 0.06219493464513641, lr: 0.000995917237285747\n",
      "epoch: 8300, accuracy:  0.940, loss:  0.265, dLoss: 0.20290579051339586, rLoss: 0.06188496544756794, lr: 0.000995867647197952\n",
      "epoch: 8400, accuracy:  0.935, loss:  0.264, dLoss: 0.20201572549330787, rLoss: 0.06157283638694674, lr: 0.0009958180620484277\n",
      "epoch: 8500, accuracy:  0.935, loss:  0.262, dLoss: 0.20119203831746446, rLoss: 0.06125616341580963, lr: 0.0009957684818364362\n",
      "epoch: 8600, accuracy:  0.935, loss:  0.261, dLoss: 0.2003995771005932, rLoss: 0.06093711214166357, lr: 0.0009957189065612402\n",
      "epoch: 8700, accuracy:  0.940, loss:  0.260, dLoss: 0.19960851981047462, rLoss: 0.060617195874545754, lr: 0.000995669336222102\n",
      "epoch: 8800, accuracy:  0.935, loss:  0.259, dLoss: 0.19882421076996415, rLoss: 0.0603012329897623, lr: 0.000995619770818285\n",
      "epoch: 8900, accuracy:  0.940, loss:  0.258, dLoss: 0.19808511774440307, rLoss: 0.059985470143227186, lr: 0.0009955702103490519\n",
      "epoch: 9000, accuracy:  0.945, loss:  0.256, dLoss: 0.1961080290008408, rLoss: 0.05976686086964069, lr: 0.000995520654813666\n",
      "epoch: 9100, accuracy:  0.940, loss:  0.255, dLoss: 0.19504682269160373, rLoss: 0.05966523093241706, lr: 0.0009954711042113903\n",
      "epoch: 9200, accuracy:  0.940, loss:  0.254, dLoss: 0.19420050400685004, rLoss: 0.059409187691332146, lr: 0.0009954215585414883\n",
      "epoch: 9300, accuracy:  0.940, loss:  0.253, dLoss: 0.1934243062961321, rLoss: 0.05912174607921636, lr: 0.000995372017803224\n",
      "epoch: 9400, accuracy:  0.940, loss:  0.251, dLoss: 0.19267060798182012, rLoss: 0.05882776075852145, lr: 0.0009953224819958604\n",
      "epoch: 9500, accuracy:  0.940, loss:  0.250, dLoss: 0.19194266866812157, rLoss: 0.05853917509586917, lr: 0.000995272951118662\n",
      "epoch: 9600, accuracy:  0.940, loss:  0.249, dLoss: 0.19124182424397104, rLoss: 0.05825017700468959, lr: 0.0009952234251708924\n",
      "epoch: 9700, accuracy:  0.940, loss:  0.249, dLoss: 0.19053690676642152, rLoss: 0.05796459967639621, lr: 0.000995173904151816\n",
      "epoch: 9800, accuracy:  0.940, loss:  0.248, dLoss: 0.1898623728532963, rLoss: 0.057681296090687534, lr: 0.0009951243880606966\n",
      "epoch: 9900, accuracy:  0.940, loss:  0.247, dLoss: 0.18919041496579567, rLoss: 0.05739732215208412, lr: 0.0009950748768967994\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That's our model! Naturally, we can tweak and tune some things here, but our model has overall performed relatively well! In the next chapter, we'll be working on using this to predict regressions!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça!"
   ],
   "id": "5fde35300d4a8aab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
