{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 9: Backpropagation",
   "id": "87878dadf1e07e2d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.285450Z",
     "start_time": "2025-06-19T17:42:12.984374Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Backprop. Intro\n",
    "\n",
    "We'll start off the chapter by backpropagating the ReLU function for a single neuron with the goal of minimizing **the output** from this neuron. This won't directly translate to our model ops, since the goal there is minimize **loss**, but it does serve as a good example showing how the process would work.\n",
    "\n",
    "Let's initialize a neuron:\n"
   ],
   "id": "23d1b57948b9cfb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.290818Z",
     "start_time": "2025-06-19T17:42:13.286605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating input list of length 3\n",
    "x = [1.0, -2.0, 3.0]\n",
    "# Creating random weights\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "# Setting bias variable\n",
    "b = 1\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# This could have been done just using a \"z = np.dot(x, w) + b\", but the format we've chosen is more convenient for our experimentation\n",
    "print(f\"the layer output before act. function is {z}\")\n",
    "\n",
    "y = max(z, 0)\n",
    "print(f\"the neuron output is {y}\")"
   ],
   "id": "9d464efddf0535ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the layer output before act. function is 6.0\n",
      "the neuron output is 6.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that was a full forward pass through the (made up) data! Now we can think about how to approach backpropagation.\n",
    "\n",
    "First, lets imagine what our function is actually doing, which can be roughly interpreted as $ReLU(\\sum[inputs * weights] + bias)$ and which we can write more specifically as $ReLU(x0w0 + x1w1 + x2w2 + bias)$. We will rewrite this as $y = ReLU(sum(mul(x0, w0), mul(x1, w1), mul(x2, w2), bias))$ for the purposes of easier derivation. If we're trying to find the derivative of y with respect to x0, we can write the following:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{0}}[ReLU(sum(mul(x0, w0), mul(x1, w1), mul(x2, w2), bias))] = \\\\\n",
    "\\frac{dReLU()}{dSum()} \\cdot \\frac{\\partial sum()}{\\partial mul(x_{0}, w_{0})} \\cdot \\frac{\\partial mul(x_{0}), w_{0}}{\\partial x_{0}}\n",
    "$$\n",
    "Now, if we were to just solve this out, we would see the impact that $x_{0}$ is actually having on the output.\n",
    "\n",
    "During the backward pass, what we actually do is calculate the derivative of the loss function and multiply it with the derivative of the activation function, and then the derivative of the output layer, and so on, all the way through the hidden layers and activation functions.\n",
    "\n",
    "In all of these layers, the derivative with respect to the weights and biases will form the gradients that will tell us how to update our weights and biases.\n",
    "\n",
    "Let's work backwards through our network now, assuming that the neuron receives a gradient of 1 from the next layer.\n",
    "\n",
    "The first step in our process is calculating the derivative of the ReLU activation function -- which we've already done before! I'll write it out below: \n",
    "$$\n",
    "f(x) = max(x, 0) \\rightarrow \\frac{d}{dx} f(x) = 1(x > 0)\n",
    "$$\n",
    "\n",
    "Now, lets move to using this in python."
   ],
   "id": "42e38a6ee35944fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.294745Z",
     "start_time": "2025-06-19T17:42:13.291855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make sure you have run the previous code cell so there is a z to go off.\n",
    "\n",
    "# Hard-coding the gradient from the previous layer\n",
    "dValue = 1.0\n",
    "\n",
    "# The RHS of the below is the derivative of the ReLU function with respect to z, because z denotes the neuron's output. \n",
    "dReluDz = dValue * (1. if (z > 0) else 0.)\n",
    "print(dReluDz)"
   ],
   "id": "6d20f158ee7adc1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now with our ReLU derivative handled, the immediately preceding operation was the summation of the weights inputs and bias. So, here we need to calculate a partial derivative of the sum function and then use the chain rule to multiply it by the derivative of the outer function -- which is the ReLU.  \n",
    "\n",
    "We can begin defining the partial derivatives:\n",
    "- dReluDxw0 -- the partial derivative of RELU w.r.t. the first weighted input, x0w0\n",
    "- dReluDxw1 -- the partial derivative of RELU w.r.t. the second weighted input, x1w1\n",
    "- dReluDxw2 -- the partial derivative of RELU w.r.t. the third weighted input, x2w2\n",
    "- dReluDb -- the partial derivative of RELU w.r.t. the bias, b\n",
    "\n",
    "As we know, the partial derivative of any sum operation is always 1, no matter what the inputs are.\n",
    "\n",
    "So, we can now incorporate this into our python."
   ],
   "id": "5212e1be08e7fdaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.298279Z",
     "start_time": "2025-06-19T17:42:13.295659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make sure you have run the previous code cells so there is a dReluDz to go off.\n",
    "\n",
    "# I'm just going to make one variable, since all of it will just be 1\n",
    "dSumDxwX = 1\n",
    "dSumDb = 1\n",
    "\n",
    "# Now let's calculate the derivative for each\n",
    "dReluDxw0 = dReluDz * dSumDxwX\n",
    "dReluDxw1 = dReluDz * dSumDxwX\n",
    "dReluDxw2 = dReluDz * dSumDxwX\n",
    "dReluDb = dReluDz * dSumDb\n",
    "\n",
    "print(dReluDxw0, dReluDxw1, dReluDxw2, dReluDb)"
   ],
   "id": "40d82922dde440c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great, so that's the summation function! Now, we have to do arguably the most complex one: the multiplication function.\n",
    "\n",
    "As we can remember, the derivative for a product is whatever the input is being multiplied by, as I'll show below:\n",
    "$$\n",
    "f(x,y) = x \\cdot y \\rightarrow \\frac{\\partial}{\\partial x} f(x,y) = y \\\\\n",
    "\\frac{\\partial}{\\partial y} f(x,y) = x \\\\\n",
    "$$\n",
    "\n",
    "Following this, the partial derivative of the first weighted input $(x \\cdot w)$ with respect to the input (x) is just the weight (w) -- as it is the other input of the function.\n",
    "\n",
    "So, let's add this functionality to our code."
   ],
   "id": "426623e6ed2e6602"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.303206Z",
     "start_time": "2025-06-19T17:42:13.300225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pull the variables\n",
    "dMulDx0 = w[0]\n",
    "dMulDx1 = w[1]\n",
    "dMulDx2 = w[2]\n",
    "dMulDw0 = x[0]\n",
    "dMulDw1 = x[1]\n",
    "dMulDw2 = x[2]\n",
    "\n",
    "# Actually calculate the derivative\n",
    "dReluDx0 = dReluDxw0 * dMulDx0\n",
    "dReluDx1 = dReluDxw1 * dMulDx1\n",
    "dReluDx2 = dReluDxw2 * dMulDx2\n",
    "dReluDw0 = dReluDxw0 * dMulDw0\n",
    "dReluDw1 = dReluDxw1 * dMulDw1\n",
    "dReluDw2 = dReluDxw2 * dMulDw2\n",
    "\n",
    "print(dReluDx0, dReluDw0, dReluDx1, dReluDw1, dReluDx2, dReluDw2)"
   ],
   "id": "e1795e6e1423a332",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that is our entire set of neuronal partial derivatives with respect to the inputs, weights, and the bias. We can now use this to optimize these calculations. \n",
    "\n",
    "All together, these can be represented as:"
   ],
   "id": "14b3cae2c0893be7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.306659Z",
     "start_time": "2025-06-19T17:42:13.304027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dx = [dReluDx0, dReluDx1, dReluDx2] # the gradients on inputs\n",
    "dw = [dReluDw0, dReluDw1, dReluDw2] # the gradients on the weights\n",
    "db = dReluDb # the gradient on the bias, of which there is just one\n",
    "\n",
    "print(dx, dw, db)"
   ],
   "id": "859fa23481b8324c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] [1.0, -2.0, 3.0] 1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We'll now use these to see how we can change our weights to minimize the output (as was our goal for this example), but we would normally use them in our optimizer to improve the output.  \n",
    "\n",
    "If we take a look at our current weights, bias, and output:"
   ],
   "id": "538e70d00373264d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.309719Z",
     "start_time": "2025-06-19T17:42:13.307491Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"{w}, {b}, {z}\")",
   "id": "638d62011428710a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0], 1, 6.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can use our calculated partial derivatives to play with this and see if we can decrease output:",
   "id": "a50970970667ca72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.313020Z",
     "start_time": "2025-06-19T17:42:13.310420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ],
   "id": "8fe5d9bf990e6ba9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.001, -0.998, 1.997] 0.999\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets perform a forward pass to see how this impacts our final output:",
   "id": "17cd85e97d0f0541"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:42:13.316124Z",
     "start_time": "2025-06-19T17:42:13.313691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Multiply inputs and weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Add up mult + bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU function for output\n",
    "y = max(z, 0)\n",
    "\n",
    "print(y)"
   ],
   "id": "5ac1eda49506a2a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.985\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That means that we've reduced our output! While it's only by a very tiny bit, 6.0 vs 5.985, it shows us that we're trending in the right direction! Like I said, optimizing a single neuron for the pure sake of minimizing it's output is something that won't translate into the real world, but it's a step. What we're actually going to be doing is working to decrease the final loss value \n",
    "\n",
    "Our next objective will be to apply this to a list of samples and expand it to a whole layer of neurons. In this example, our neural net will consist of a single hidden layer with 3 neurons (each with 3 inputs and 3 weights). Let's set up below:"
   ],
   "id": "29bacdd8e46db942"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:02:29.942014Z",
     "start_time": "2025-06-19T18:02:29.937519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We'll make up the gradients from the \"next\" layer for the sake of this example\n",
    "dvalues = np.array([[1.0, 1.0, 1.0]])\n",
    "\n",
    "# We have 3 sets of weights and 4 inputs, meaning we need 4 weights each.\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                   [0.5, -0.91, 0.26, -0.5],\n",
    "                   [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# Sum the weights of inputs and multipy by the gradients\n",
    "dx0 = sum(weights[0]*dvalues[0])\n",
    "dx1 = sum(weights[1]*dvalues[0])\n",
    "dx2 = sum(weights[2]*dvalues[0])\n",
    "dx3 = sum(weights[3]*dvalues[0])\n",
    "\n",
    "dInputs = np.array([dx0, dx1, dx2, dx3])\n",
    "\n",
    "print(dInputs)"
   ],
   "id": "f1c67e56cf2d1084",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From this, we see how dInputs is the gradient of the neuron function with respect to the outputs.\n",
    "\n",
    "However, we can simplify this tremendously by just using np.dot!  "
   ],
   "id": "a609ecf6118dcec9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:02:32.147240Z",
     "start_time": "2025-06-19T18:02:32.143375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dInputs = np.dot(dvalues[0], weights.T)\n",
    "print(dInputs)"
   ],
   "id": "3d92732e0c0ac14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That about does it -- but we're missing one thing: the ability to handle samples in our batch. Let's implement that now:",
   "id": "a081a418bb434e1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:04:27.141643Z",
     "start_time": "2025-06-19T18:04:27.132824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We'll create gradient values for each batch\n",
    "dvalues = np.array([[1.0, 1.0, 1.0],\n",
    "                    [2.0, 2.0, 2.0],\n",
    "                    [3.0, 3.0, 3.0]])\n",
    "\n",
    "dInputs = np.dot(dvalues, weights.T)\n",
    "\n",
    "print(dInputs)"
   ],
   "id": "d0f4b034573ecd9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Those are our gradients with respect to the inputs. That was a lot. So, now we should take a look at our gradients with respect to the weights. ",
   "id": "5629e58621ebad71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:18:09.211146Z",
     "start_time": "2025-06-19T18:18:09.205297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We have 3 sets of sample inputs\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2, 5, -1, 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Notice how this time we flip the position of inputs.T and dvalues so that the arrangement is (n x m) and (m x p).\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "\n",
    "print(dweights)"
   ],
   "id": "cae7a4ad035fc91f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This correspondingly matches our shape of weights because we've summed the inputs for each weight and then multipled it by the input gradient. We can do this for biases as well!",
   "id": "cacbac4b075a13ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:23:19.257611Z",
     "start_time": "2025-06-19T18:23:19.249365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One bias for each neuron\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Sum it over the samples and keep the row vector dimensions\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "print(dbiases)"
   ],
   "id": "4f83fad42afc7d58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we should also account for the ReLU function, which is 1 when > 0, 0 otherwise.",
   "id": "4c2d2c139d09df4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T18:28:46.365994Z",
     "start_time": "2025-06-19T18:28:46.358043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a random array of layer outputs\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "              [2, -7, -1, 3],\n",
    "              [-1, 2, 5, -1]])\n",
    "\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# np.zeros_like(arg) is a function that returns an array of the same size as the arg but filled with 0's\n",
    "drelu = np.zeros_like(z)\n",
    "# This iterates through the elements and if z > 0, sets it to 1.\n",
    "drelu[z > 0] = 1\n",
    "print(drelu)\n",
    "\n",
    "# Apply the chain rule\n",
    "drelu *= dvalues\n",
    "print(drelu)"
   ],
   "id": "70df530a457c0e81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I'm going to update our classes to account for what we've learned so far in this chapter, but I'm going to detail everything to check out from those changes:\n",
    "- Within the DenseLayer class:\n",
    "    - Added \"self.inputs\" as a object in the forward method\n",
    "    - Created the \"backward\" method and its corresponding process\n",
    "- Within the ReLU class:\n",
    "    - Added \"self.inputs\" as a object in the forward method\n",
    "    - Created the \"backward\" method and its corresponding process\n",
    "\n",
    "## Section 2: Categorical Cross-Entropy loss derivatives"
   ],
   "id": "d85819c713c3872f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
