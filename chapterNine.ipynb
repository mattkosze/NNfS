{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 9: Backpropagation",
   "id": "87878dadf1e07e2d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T23:20:41.997519Z",
     "start_time": "2025-06-19T23:20:41.988070Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from timeit import timeit\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy, SoftMaxCategoricalCrossEntropy"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Backprop. Intro\n",
    "\n",
    "We'll start off the chapter by backpropagating the ReLU function for a single neuron with the goal of minimizing **the output** from this neuron. This won't directly translate to our model ops, since the goal there is minimize **loss**, but it does serve as a good example showing how the process would work.\n",
    "\n",
    "Let's initialize a neuron:\n"
   ],
   "id": "23d1b57948b9cfb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating input list of length 3\n",
    "x = [1.0, -2.0, 3.0]\n",
    "# Creating random weights\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "# Setting bias variable\n",
    "b = 1\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# This could have been done just using a \"z = np.dot(x, w) + b\", but the format we've chosen is more convenient for our experimentation\n",
    "print(f\"the layer output before act. function is {z}\")\n",
    "\n",
    "y = max(z, 0)\n",
    "print(f\"the neuron output is {y}\")"
   ],
   "id": "9d464efddf0535ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that was a full forward pass through the (made up) data! Now we can think about how to approach backpropagation.\n",
    "\n",
    "First, lets imagine what our function is actually doing, which can be roughly interpreted as $ReLU(\\sum[inputs * weights] + bias)$ and which we can write more specifically as $ReLU(x0w0 + x1w1 + x2w2 + bias)$. We will rewrite this as $y = ReLU(sum(mul(x0, w0), mul(x1, w1), mul(x2, w2), bias))$ for the purposes of easier derivation. If we're trying to find the derivative of y with respect to x0, we can write the following:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{0}}[ReLU(sum(mul(x0, w0), mul(x1, w1), mul(x2, w2), bias))] = \\\\\n",
    "\\frac{dReLU()}{dSum()} \\cdot \\frac{\\partial sum()}{\\partial mul(x_{0}, w_{0})} \\cdot \\frac{\\partial mul(x_{0}), w_{0}}{\\partial x_{0}}\n",
    "$$\n",
    "Now, if we were to just solve this out, we would see the impact that $x_{0}$ is actually having on the output.\n",
    "\n",
    "During the backward pass, what we actually do is calculate the derivative of the loss function and multiply it with the derivative of the activation function, and then the derivative of the output layer, and so on, all the way through the hidden layers and activation functions.\n",
    "\n",
    "In all of these layers, the derivative with respect to the weights and biases will form the gradients that will tell us how to update our weights and biases.\n",
    "\n",
    "Let's work backwards through our network now, assuming that the neuron receives a gradient of 1 from the next layer.\n",
    "\n",
    "The first step in our process is calculating the derivative of the ReLU activation function -- which we've already done before! I'll write it out below: \n",
    "$$\n",
    "f(x) = max(x, 0) \\rightarrow \\frac{d}{dx} f(x) = 1(x > 0)\n",
    "$$\n",
    "\n",
    "Now, lets move to using this in python."
   ],
   "id": "42e38a6ee35944fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make sure you have run the previous code cell so there is a z to go off.\n",
    "\n",
    "# Hard-coding the gradient from the previous layer\n",
    "dValue = 1.0\n",
    "\n",
    "# The RHS of the below is the derivative of the ReLU function with respect to z, because z denotes the neuron's output. \n",
    "dReluDz = dValue * (1. if (z > 0) else 0.)\n",
    "print(dReluDz)"
   ],
   "id": "6d20f158ee7adc1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now with our ReLU derivative handled, the immediately preceding operation was the summation of the weights inputs and bias. So, here we need to calculate a partial derivative of the sum function and then use the chain rule to multiply it by the derivative of the outer function -- which is the ReLU.  \n",
    "\n",
    "We can begin defining the partial derivatives:\n",
    "- dReluDxw0 -- the partial derivative of RELU w.r.t. the first weighted input, x0w0\n",
    "- dReluDxw1 -- the partial derivative of RELU w.r.t. the second weighted input, x1w1\n",
    "- dReluDxw2 -- the partial derivative of RELU w.r.t. the third weighted input, x2w2\n",
    "- dReluDb -- the partial derivative of RELU w.r.t. the bias, b\n",
    "\n",
    "As we know, the partial derivative of any sum operation is always 1, no matter what the inputs are.\n",
    "\n",
    "So, we can now incorporate this into our python."
   ],
   "id": "5212e1be08e7fdaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make sure you have run the previous code cells so there is a dReluDz to go off.\n",
    "\n",
    "# I'm just going to make one variable, since all of it will just be 1\n",
    "dSumDxwX = 1\n",
    "dSumDb = 1\n",
    "\n",
    "# Now let's calculate the derivative for each\n",
    "dReluDxw0 = dReluDz * dSumDxwX\n",
    "dReluDxw1 = dReluDz * dSumDxwX\n",
    "dReluDxw2 = dReluDz * dSumDxwX\n",
    "dReluDb = dReluDz * dSumDb\n",
    "\n",
    "print(dReluDxw0, dReluDxw1, dReluDxw2, dReluDb)"
   ],
   "id": "40d82922dde440c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great, so that's the summation function! Now, we have to do arguably the most complex one: the multiplication function.\n",
    "\n",
    "As we can remember, the derivative for a product is whatever the input is being multiplied by, as I'll show below:\n",
    "$$\n",
    "f(x,y) = x \\cdot y \\rightarrow \\frac{\\partial}{\\partial x} f(x,y) = y \\\\\n",
    "\\frac{\\partial}{\\partial y} f(x,y) = x \\\\\n",
    "$$\n",
    "\n",
    "Following this, the partial derivative of the first weighted input $(x \\cdot w)$ with respect to the input (x) is just the weight (w) -- as it is the other input of the function.\n",
    "\n",
    "So, let's add this functionality to our code."
   ],
   "id": "426623e6ed2e6602"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pull the variables\n",
    "dMulDx0 = w[0]\n",
    "dMulDx1 = w[1]\n",
    "dMulDx2 = w[2]\n",
    "dMulDw0 = x[0]\n",
    "dMulDw1 = x[1]\n",
    "dMulDw2 = x[2]\n",
    "\n",
    "# Actually calculate the derivative\n",
    "dReluDx0 = dReluDxw0 * dMulDx0\n",
    "dReluDx1 = dReluDxw1 * dMulDx1\n",
    "dReluDx2 = dReluDxw2 * dMulDx2\n",
    "dReluDw0 = dReluDxw0 * dMulDw0\n",
    "dReluDw1 = dReluDxw1 * dMulDw1\n",
    "dReluDw2 = dReluDxw2 * dMulDw2\n",
    "\n",
    "print(dReluDx0, dReluDw0, dReluDx1, dReluDw1, dReluDx2, dReluDw2)"
   ],
   "id": "e1795e6e1423a332",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that is our entire set of neuronal partial derivatives with respect to the inputs, weights, and the bias. We can now use this to optimize these calculations. \n",
    "\n",
    "All together, these can be represented as:"
   ],
   "id": "14b3cae2c0893be7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dx = [dReluDx0, dReluDx1, dReluDx2] # the gradients on inputs\n",
    "dw = [dReluDw0, dReluDw1, dReluDw2] # the gradients on the weights\n",
    "db = dReluDb # the gradient on the bias, of which there is just one\n",
    "\n",
    "print(dx, dw, db)"
   ],
   "id": "859fa23481b8324c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We'll now use these to see how we can change our weights to minimize the output (as was our goal for this example), but we would normally use them in our optimizer to improve the output.  \n",
    "\n",
    "If we take a look at our current weights, bias, and output:"
   ],
   "id": "538e70d00373264d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"{w}, {b}, {z}\")",
   "id": "638d62011428710a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can use our calculated partial derivatives to play with this and see if we can decrease output:",
   "id": "a50970970667ca72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ],
   "id": "8fe5d9bf990e6ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets perform a forward pass to see how this impacts our final output:",
   "id": "17cd85e97d0f0541"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Multiply inputs and weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Add up mult + bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU function for output\n",
    "y = max(z, 0)\n",
    "\n",
    "print(y)"
   ],
   "id": "5ac1eda49506a2a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That means that we've reduced our output! While it's only by a very tiny bit, 6.0 vs 5.985, it shows us that we're trending in the right direction! Like I said, optimizing a single neuron for the pure sake of minimizing it's output is something that won't translate into the real world, but it's a step. What we're actually going to be doing is working to decrease the final loss value \n",
    "\n",
    "Our next objective will be to apply this to a list of samples and expand it to a whole layer of neurons. In this example, our neural net will consist of a single hidden layer with 3 neurons (each with 3 inputs and 3 weights). Let's set up below:"
   ],
   "id": "29bacdd8e46db942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We'll make up the gradients from the \"next\" layer for the sake of this example\n",
    "dvalues = np.array([[1.0, 1.0, 1.0]])\n",
    "\n",
    "# We have 3 sets of weights and 4 inputs, meaning we need 4 weights each.\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                   [0.5, -0.91, 0.26, -0.5],\n",
    "                   [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# Sum the weights of inputs and multipy by the gradients\n",
    "dx0 = sum(weights[0]*dvalues[0])\n",
    "dx1 = sum(weights[1]*dvalues[0])\n",
    "dx2 = sum(weights[2]*dvalues[0])\n",
    "dx3 = sum(weights[3]*dvalues[0])\n",
    "\n",
    "dInputs = np.array([dx0, dx1, dx2, dx3])\n",
    "\n",
    "print(dInputs)"
   ],
   "id": "f1c67e56cf2d1084",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From this, we see how dInputs is the gradient of the neuron function with respect to the outputs.\n",
    "\n",
    "However, we can simplify this tremendously by just using np.dot!  "
   ],
   "id": "a609ecf6118dcec9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dInputs = np.dot(dvalues[0], weights.T)\n",
    "print(dInputs)"
   ],
   "id": "3d92732e0c0ac14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That about does it -- but we're missing one thing: the ability to handle samples in our batch. Let's implement that now:",
   "id": "a081a418bb434e1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We'll create gradient values for each batch\n",
    "dvalues = np.array([[1.0, 1.0, 1.0],\n",
    "                    [2.0, 2.0, 2.0],\n",
    "                    [3.0, 3.0, 3.0]])\n",
    "\n",
    "dInputs = np.dot(dvalues, weights.T)\n",
    "\n",
    "print(dInputs)"
   ],
   "id": "d0f4b034573ecd9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Those are our gradients with respect to the inputs. That was a lot. So, now we should take a look at our gradients with respect to the weights. ",
   "id": "5629e58621ebad71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We have 3 sets of sample inputs\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2, 5, -1, 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Notice how this time we flip the position of inputs.T and dvalues so that the arrangement is (n x m) and (m x p).\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "\n",
    "print(dweights)"
   ],
   "id": "cae7a4ad035fc91f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This correspondingly matches our shape of weights because we've summed the inputs for each weight and then multipled it by the input gradient. We can do this for biases as well!",
   "id": "cacbac4b075a13ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# One bias for each neuron\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Sum it over the samples and keep the row vector dimensions\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "print(dbiases)"
   ],
   "id": "4f83fad42afc7d58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we should also account for the ReLU function, which is 1 when > 0, 0 otherwise.",
   "id": "4c2d2c139d09df4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating a random array of layer outputs\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "              [2, -7, -1, 3],\n",
    "              [-1, 2, 5, -1]])\n",
    "\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# np.zeros_like(arg) is a function that returns an array of the same size as the arg but filled with 0's\n",
    "drelu = np.zeros_like(z)\n",
    "# This iterates through the elements and if z > 0, sets it to 1.\n",
    "drelu[z > 0] = 1\n",
    "print(drelu)\n",
    "\n",
    "# Apply the chain rule\n",
    "drelu *= dvalues\n",
    "print(drelu)"
   ],
   "id": "70df530a457c0e81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I'm going to update our classes to account for what we've learned so far in this chapter, but I'm going to detail everything to check out from those changes:\n",
    "- Within the DenseLayer class:\n",
    "    - Added \"self.inputs\" as a object in the forward method\n",
    "    - Created the \"backward\" method and its corresponding process\n",
    "- Within the ReLU class:\n",
    "    - Added \"self.inputs\" as a object in the forward method\n",
    "    - Created the \"backward\" method and its corresponding process\n",
    "\n",
    "## Section 2: Categorical Cross-Entropy Loss Derivatives\n",
    "\n",
    "As seen in chapter five, the equation for the categorical cross-entropy loss function is:\n",
    "$$\n",
    "L_{i} = -log(\\hat{y}_{i,k})\n",
    "$$\n",
    "This is convenient for calculations, but for the sake of our process of finding the derivative, we will us the full equation:\n",
    "$$\n",
    "L_{i} = \\sum_{j} y_{i, j}log(\\hat{y}_{i,j})\n",
    "$$\n",
    "The reason we use the latter is because the goal of finding the gradient means that we need the partial derivatives of the loss function with respect to **each** of it's inputs, meaning we can't use the shorted version.\n",
    "\n",
    "Let us define the full equation as being:\n",
    "$$\n",
    "\\frac{\\partial L_{i}}{\\partial \\hat{y}_{i,j}}\n",
    "= \n",
    "\\frac{\\partial}{\\partial \\hat{y}_{i,j}}\n",
    "[-\\sum_{j} y_{i,j}log(\\hat{y}_{i,j})]\n",
    "$$\n",
    "\n",
    "As we know that we can remove constants, let us rewrite the equation as follows:\n",
    "$$\n",
    "-\\sum_{j} y_{i,j} \\cdot \\frac{\\partial}{\\partial \\hat{y}_{i,j}} log(\\hat{y_{i,j}})\n",
    "$$\n",
    "\n",
    "I'm going to skip re-writing some steps, but this finally solves to\n",
    "$$\n",
    " -\\frac{y_{i,j}}{\\hat{y}_{i,j}}\n",
    "$$\n",
    "\n",
    "Great - now we know what the derivative looks like and why! I'm updating the classes.py file, and I've modified the following things:\n",
    "- Created the \"backward\" method in the categorical cross entropy loss class. It basically works by first turning numerical labels into one-hot encoded vectors; then performs gradient normalization to prevent us from having to change the learning rate for each sample. \n",
    "\n",
    "## Section 3: Softmax Activation Derivative \n",
    "\n",
    "Let's write out the formula for the softmax equation to stir our brains:\n",
    "$$\n",
    "S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}\n",
    "\\rightarrow\n",
    "\\frac{\\partial S_{i,j}}{\\partial z_{i,k}}\n",
    "=\n",
    "\\frac{\\partial \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}}{\\partial z_{i,k}}\n",
    "$$\n",
    "Where $S_{i,j}$ denotes the j-th Softmax's output of the i-th sample, z -- input array which is a list of input vectors, $z_{i,j}$ -- j'th Softmax's input of i'th sample, L -- number of inputs, $z_{i,k}$ -- k'th Sotmax's input of i'th sample.\n",
    "\n",
    "Here, we're basically calculating the Jacobian matrix of the vectors, as we're trying to find the partial derivative of every output with respect to each input.\n",
    "\n",
    "Let's remember the quotient rule:\n",
    "$$\n",
    "f(x) = \\frac{g(x)}{h(x)} \\rightarrow f'(x) = \\frac{g'(x) \\times h(x) - g(x) \\times h'(x)}{[h(x)]^{2}}\n",
    "$$ \n",
    "\n",
    "For my sake and yours, I'm going to skip a lot of the math here, but the precursor you need to know is the Kronecker delta function:\n",
    "$$\n",
    "\\delta_{i,j} = \\begin{cases}\n",
    "1 \\text{ if } i = j \\\\\n",
    "0 \\text{ if } i \\neq j \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "We can then use this to furthest simplify our equation to:\n",
    "$$\n",
    "\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} = S_{i,j} \\cdot (\\delta_{i,j} - S_{i,k})\n",
    "$$\n",
    "For ease of implementation in python, we can re-write this as:\n",
    "$$\n",
    "\\frac{\\partial S_{i,j}}{\\partial z_{i,k}}\n",
    "= \n",
    "S_{i,j}\\delta_{j,k} - S_{i,j}S_{i,k}\n",
    "$$\n",
    "\n",
    "The above lets us do this in just two numpy functions! Let me show below."
   ],
   "id": "d85819c713c3872f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating a random softmax output\n",
    "example_softmax = np.array([0.7, 0.1, 0.2]).reshape(-1,1)\n",
    "\n",
    "# Creates a generic I matrix with 1's along the diagonal and 0's elsewhere\n",
    "identity = np.eye(example_softmax.shape[0])\n",
    "\n",
    "# Multiply... self explanatory.\n",
    "print(f\"Method one: \\n{identity * example_softmax}\")\n",
    "\n",
    "# BUT: This can be simplified into just one np.diagflat() call!\n",
    "\n",
    "print(f\"Method two: \\n{np.diagflat(example_softmax)}\")"
   ],
   "id": "43cee774416f4aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, the above takes care of the first $S_{i,j}\\delta_{j,k}$ part, but we're still left with the $S_{i,j}S_{i,k}$ part. That means, we just have to do x dot x.T, which I'll show below: ",
   "id": "1dda32efeef017ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(np.dot(example_softmax, example_softmax.T))",
   "id": "cd81350143f5510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we can go and perform subtraction of both arrays:",
   "id": "3fcfc5983d22aea7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(np.diagflat(example_softmax) - np.dot(example_softmax, example_softmax.T))",
   "id": "7daa7675d4e53852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above matrix is our Jacobian matrix -- which is our array of partial derivatives of in all combinations of both input vectors. We calculate the partial derivative of every output of the Softmax function with respect to each input separately because each input influences each output because of the normalization process.  \n",
    "\n",
    "This results in a list of Jacobian matrices which effectively forms a 3D matrix. That can be visualized as a column whose levels are Jacobian matrices being the sample-wise gradient of the Softmax function.\n",
    "\n",
    "I'm going to go and add some more stuff to our classes.py, which I'll explain here:\n",
    "- Added the \"backward\" method of the SoftMax class, which works by: 1. creating an empty array with the same shape as the gradients to be prepared for our application of the chain rule; 2. iterating sample-wise over the pairs of outputs and gradients, calculating the partial derivatives, final product, and gradient vector in the process, storing them in each row as we go. \n",
    "\n",
    "Interesting side note: **every** gradient is a Jacobian, but **not every** Jacobian is a gradient.\n",
    "\n",
    "## Section 4: Common Categorical Cross Entropy Loss and Softmax Activation Derivative\n",
    "\n",
    "We can combine the previous two sections to present a unified derivative: the derivative of the loss function with respect to the softmax inputs. We can define by applying the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L_{i}}{\\partial z_{i,k}}\n",
    "=\n",
    "\\frac{\\partial L_{i}}{\\partial \\hat{Y}_{i,j}} \\cdot \\frac{\\partial S_{i,j}}{\\partial z_{i,k}}\n",
    "$$\n",
    "\n",
    "This partial derivative formula is just the partial derivative of the loss function with respect to its inputs, multiplied (using the chain rule) times the partial derivative of the activation function with respect its inputs.\n",
    "\n",
    "I'll skip ALL the calculus because it's not necessary for the purposes of these public notes, but I would recommend going to the book and reading through the section.\n",
    "\n",
    "In the end, this simplifies down to:\n",
    "$$\n",
    "\\hat{y}_{i,k} - y_{i,k} \n",
    "$$ "
   ],
   "id": "6d43fdcbe742f93e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is drastically simpler and more efficient than the two discrete partial derivatives we were using before. As such, I'm going to create or update our classes in the class.py file as such:\n",
    "- Created new \"SoftMaxCategoricalCrossentropy\" class which combines both of the SoftMax and CategoricalCrossEntropy classes.\n",
    "- Implemented the forward and backward passes of that class, which make use of the methods from the extended classes for efficiency. \n",
    "\n",
    "Our solution is so much more efficient because it takes advantage of y_true being one-hot encoded, which means that, for each sample, there is only a singular 1 in those vectors and the remaining positions are filled with 0's. \n",
    "\n",
    "Finally! We can test if our combined backward step returns the same values compared to when we backpropagate gradients through both of the functions separately. For this test, we'll use random values."
   ],
   "id": "671833d7627de395"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:17:32.987734Z",
     "start_time": "2025-06-19T23:17:31.734968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initializing random data generator\n",
    "nnfs.init()\n",
    "\n",
    "# Some random softmax outputs, each row adding up to 1\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "# Creating ground truth labels\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Our combined method\n",
    "def f1():\n",
    "    softmaxLoss = SoftMaxCategoricalCrossEntropy()\n",
    "    softmaxLoss.backward(softmax_outputs, class_targets)\n",
    "    dvalues1 = softmaxLoss.dinputs\n",
    "    return dvalues1\n",
    "\n",
    "# Our separate method\n",
    "def f2():\n",
    "    activation = SoftMax()\n",
    "    activation.output = softmax_outputs\n",
    "    loss = CategoricalCrossEntropy()\n",
    "    loss.backward(softmax_outputs, class_targets)\n",
    "    activation.backward(loss.dinputs)\n",
    "    dvalues2 = activation.dinputs\n",
    "    return dvalues2\n",
    "    \n",
    "dvalues1 = f1()\n",
    "dvalues2 = f2()\n",
    "\n",
    "t1 = timeit(lambda: f1(), number=10000)\n",
    "t2 = timeit(lambda: f2(), number=10000)\n",
    "\n",
    "print(f\"Gradients: combined loss and activation: {dvalues1}\")\n",
    "print(f\"Gradients: separate loss and activation: {dvalues2}\")\n",
    "\n",
    "print(f\"F2 takes {t2/t1} times more than F1!!!\")\n"
   ],
   "id": "c7d11c748286402f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: combined loss and activation: [[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: separate loss and activation: [[-0.09999999  0.03333334  0.06666667]\n",
      " [ 0.03333334 -0.16666667  0.13333334]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "F2 takes 17.969141402203054 times more than F1!!!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, aside from an ultra-minor rounding difference (resulting from the precision of floating points in raw Python vs NumPy) these are the same outputs!! However, as we see on the outputs, F2 takes significantly longer than F1, due to it's lack of efficiency optimizations.\n",
    "\n",
    "With the above in mind, we can finally code up a final version of the model in this chapter!"
   ],
   "id": "5594a721e5da7345"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T23:27:29.503723Z",
     "start_time": "2025-06-19T23:27:29.491684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize data\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create dense layer with 2 input features and 3 output values\n",
    "dense1 = DenseLayer(2, 3)\n",
    "\n",
    "# Add a ReLU activation function to be used on this\n",
    "activation1 = ReLU()\n",
    "\n",
    "# Create a 2nd dense layer with 3 input features and 3 output values\n",
    "dense2 = DenseLayer(3, 3)\n",
    "\n",
    "# Create a softmax classifier for the output\n",
    "activation2 = SoftMaxCategoricalCrossEntropy()\n",
    "\n",
    "# Forward pass through first layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Apply ReLU to output of Dense1\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Forward pass through second layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Calculate loss\n",
    "loss = activation2.forward(dense2.output, y)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Backward pass\n",
    "activation2.backward(activation2.output, y)\n",
    "dense2.backward(activation2.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Printing gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ],
   "id": "a76e037a6c2422ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333355 0.3333332  0.3333332 ]\n",
      " [0.33333382 0.33333313 0.3333331 ]\n",
      " [0.3333341  0.33333302 0.33333296]\n",
      " [0.33333433 0.3333329  0.33333278]]\n",
      "Loss: 1.098608136177063\n",
      "Accuracy: 0.33666666666666667\n",
      "[[ 3.3042468e-06 -3.9488241e-06 -9.9410368e-05]\n",
      " [-2.2006872e-05  3.0671345e-04  1.6974623e-04]]\n",
      "[[-1.8163288e-05 -5.1999162e-04  1.4667885e-05]]\n",
      "[[ 9.1446236e-05 -2.5220116e-04  1.6075492e-04]\n",
      " [-1.7278348e-04  3.9700870e-04 -2.2422522e-04]\n",
      " [ 4.4883702e-05 -1.2783038e-04  8.2946674e-05]]\n",
      "[[ 4.6612695e-06 -8.3957566e-06  3.5986304e-06]]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The final model compilation really helped solidify everything into place for me. We can now actually track model performance and almost use it to tune our parameters in hope of improving it -- and that's what we'll cover in the next chapter of the book: optimizers!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça!"
   ],
   "id": "cb57afa988ecfea3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
