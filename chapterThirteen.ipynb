{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 13: Training Datasets\n",
    "\n",
    "Before feeding a model data, it is common to do pre-processing on said data. This is most noted during the training process, but it must be said that any pre-processing done on training data should also be done on any test, validation, and later inference data.\n",
    "\n",
    "Neural networks usually perform the best on data consisting of numbers in the range of 0 to 1 or -1 to 1, with the latter being preferred. Centering the data on a value of 0 can help with model training as it attentuates weight biasing in some direction. Models can work okay with data in the range of 0 to 1, but rescaling them to -1 to 1 makes them work ever better. The benefits of keeping our numbers in this range are numerous, but include: smaller likelihood of overflows or gradient explosions, better performance in activation functions, etc...\n",
    "\n",
    "There are many methods for preprocessing: standardization, scaling, variance scaling, mean removal, non-linear transformations, and so on... but they are not covered in the NNfs book. The only thing we will be doing here is scale the data from 0 to 1 by dividing every entry by the max, and or scaling it from -1 to 1 by first subtracting by max/2 then dividing each entry by max/2.\n",
    "\n",
    "We need to ensure identical scaling on all datasets. So, when we do the above practice, we must be sure that the scaler which we are dividing by is indeed the largest we will see, so it can accurately be 1. From there, we must save the scalar with model so it can be applied in the future during inference, as that data must be scaled the same way for it to be relevant. \n",
    "\n",
    "If we don't have enough data, we can do something called data augmentation. That means, taking images already in the dataset and modifying them such that they are different from the training data (via cropping, recolouring, adding noise, etc...) and can be used as new, discrete, data.\n",
    "\n",
    "That was another short chapter!!\n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça!"
   ],
   "id": "4c9d2948e6f8ce10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
