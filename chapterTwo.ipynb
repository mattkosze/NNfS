{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 2: (There's no Chapter 1...)",
   "id": "fa8fb1ba6283921b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem 1: A Single Neuron\n",
    "In this problem we're basically building out a single neuron to get an intuition about how it works. I was debating doing this part out, but I think it can't hurt and foundations are super important IMO.\n",
    "\n",
    "Fundamentally, the output of any singular neuron is determined by three things: inputs, weights, and bias. The inputs are naturally kept the same, but weights and biases change as the model trains. Every neuron will have n weights corresponding to the n incoming connections coming from the previous layer. These are then summed in the neuron and a bias is then added to make the neuron generalize better. Then there's something about activation functions, but we're not there yet.\n",
    "\n",
    "Side note: it's fascinating to see how this connects to the human brain."
   ],
   "id": "9f970cb8cda44f5b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T04:24:34.373007Z",
     "start_time": "2025-06-15T04:24:34.370370Z"
    }
   },
   "source": [
    "# Given a singular neuron with X incoming connections...\n",
    "\n",
    "inputs = [2, 4, 6] # of length X, 1 for every incoming connection\n",
    "weights = [.8, .75, .3] # also of length X, 1 for every incoming connection\n",
    "bias = 1.5 # just a singular value, as there's only 1 per neuron\n",
    "\n",
    "# the below is the neuron output, which takes the form: Output = (Inputs * Weights) + Bias\n",
    "output = ((inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2]) + bias)\n",
    "\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.8999999999999995\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This may not look like much... because it really isn't. But it is a nice intuition that will let us build up on the notion of neural networks (not too far!) down the line.",
   "id": "6b80da86475e4322"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem 2: A Layer of Neurons\n",
    "> The book says: \"neural networks typically have layers that consist of more than one neuron.\"\n",
    "\n",
    "Each neuron in a layer gets the same input, BUT each neuron consists of its own of weights and bias, meaning each will (barring the same weights) produce different outputs.\n",
    "\n",
    "So, let's build a true \"layer\" of neurons. "
   ],
   "id": "f3d8bf34d0543f01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T04:24:34.380145Z",
     "start_time": "2025-06-15T04:24:34.374686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given a dense layer with 3 neurons each with in-degree 4 (meaning four incoming connections from the previous layer)...\n",
    "\n",
    "inputs = [1, 3, 4, 2] # of length 4, given the 4 inputs\n",
    "\n",
    "# weights of length 4, because 3 neurons each with 4 in-degree.\n",
    "weights = [[.1, .3, .2, .75], [.9, .1, 1, -.3], [-.4, .6, .2, 1]]\n",
    "\n",
    "# 3 singular values; 1 per neuron\n",
    "biases = [2, 3, .5]\n",
    "\n",
    "# the below are the outputs of the layer. we basically just do matrix multiplication and we're definitely going to be doing this using numpy in the near future. For anyone interested: this has a variety of advantages, it's extremely efficient. Look into it!\n",
    "outputs = [\n",
    "    inputs[0]*weights[0][0] + inputs[1]*weights[0][1] + inputs[2]*weights[0][2] + inputs[3]*weights[0][3] + biases[0],\n",
    "    inputs[0]*weights[1][0] + inputs[1]*weights[1][1] + inputs[2]*weights[1][2] + inputs[3]*weights[1][3] + biases[1],\n",
    "    inputs[0]*weights[2][0] + inputs[1]*weights[2][1] + inputs[2]*weights[2][2] + inputs[3]*weights[2][3] + biases[2],\n",
    "]\n",
    "\n",
    "print(f\"Statically calculated version: {outputs}\")\n",
    "\n",
    "# As I said, the way above doesn't really scale, but we can make it a little bit better with the use of loops to dynamically do this.\n",
    "outputs = []\n",
    "for neuronWeights, neuronBias in zip(weights, biases):\n",
    "    neuronOutput = 0\n",
    "    for nInput, weight in zip(inputs, neuronWeights):\n",
    "        neuronOutput += nInput * weight\n",
    "    neuronOutput += neuronBias\n",
    "    outputs.append(neuronOutput)\n",
    "        \n",
    "print(f\"Dynamically calculated version: {outputs}\")"
   ],
   "id": "1cc868d75f30280b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statically calculated version: [5.3, 7.6000000000000005, 4.7]\n",
      "Dynamically calculated version: [5.3, 7.6000000000000005, 4.7]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above is technically called a \"fully connected\" neural network - where every neuron in the current layer has a connection to each neuron in the previous layer. \n",
    "\n",
    "The number of neurons you use in each layer is totally up to you, and we'll find out throughout the course of the book what can influence your choices there. "
   ],
   "id": "923debb8b81bbf94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
