{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 2: (There's no Chapter 1...)",
   "id": "fa8fb1ba6283921b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.239731Z",
     "start_time": "2025-06-15T20:39:34.237534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np"
   ],
   "id": "10fd8dd2ba4e4712",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: A Single Neuron\n",
    "In this problem we're basically building out a single neuron to get an intuition about how it works. I was debating doing this part out, but I think it can't hurt and foundations are super important IMO.\n",
    "\n",
    "Fundamentally, the output of any singular neuron is determined by three things: inputs, weights, and bias. The inputs are naturally kept the same, but weights and biases change as the model trains. Every neuron will have n weights corresponding to the n incoming connections coming from the previous layer. These are then summed in the neuron and a bias is then added to make the neuron generalize better. Then there's something about activation functions, but we're not there yet.\n",
    "\n",
    "Side note: it's fascinating to see how this connects to the human brain."
   ],
   "id": "9f970cb8cda44f5b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.267739Z",
     "start_time": "2025-06-15T20:39:34.264648Z"
    }
   },
   "source": [
    "# Given a singular neuron with X incoming connections...\n",
    "\n",
    "inputs = [2, 4, 6] # of length X, 1 for every incoming connection\n",
    "weights = [.8, .75, .3] # also of length X, 1 for every incoming connection\n",
    "bias = 1.5 # just a singular value, as there's only 1 per neuron\n",
    "\n",
    "# the below is the neuron output, which takes the form: Output = (Inputs * Weights) + Bias\n",
    "output = ((inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2]) + bias)\n",
    "\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.8999999999999995\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This may not look like much... because it really isn't. But it is a nice intuition that will let us build up on the notion of neural networks (not too far!) down the line.",
   "id": "6b80da86475e4322"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 2: A Layer of Neurons\n",
    "> The book says: \"neural networks typically have layers that consist of more than one neuron.\" Fascinating.\n",
    "\n",
    "Each neuron in a layer gets the same input, BUT each neuron consists of its own of weights and bias, meaning each will (barring the same weights) produce different outputs.\n",
    "\n",
    "So, let's build a true \"layer\" of neurons. "
   ],
   "id": "f3d8bf34d0543f01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.280558Z",
     "start_time": "2025-06-15T20:39:34.273181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given a dense layer with 3 neurons each with in-degree 4 (meaning four incoming connections from the previous layer)...\n",
    "\n",
    "inputs = [1, 3, 4, 2] # of length 4, given the 4 inputs\n",
    "\n",
    "# weights of length 4, because 3 neurons each with 4 in-degree.\n",
    "weights = [[.1, .3, .2, .75], [.9, .1, 1, -.3], [-.4, .6, .2, 1]]\n",
    "\n",
    "# 3 singular values; 1 per neuron\n",
    "biases = [2, 3, .5]\n",
    "\n",
    "# the below are the outputs of the layer. we basically just do matrix multiplication and we're definitely going to be doing this using numpy in the near future. For anyone interested: this has a variety of advantages, it's extremely efficient. Look into it!\n",
    "outputs = [\n",
    "    inputs[0]*weights[0][0] + inputs[1]*weights[0][1] + inputs[2]*weights[0][2] + inputs[3]*weights[0][3] + biases[0],\n",
    "    inputs[0]*weights[1][0] + inputs[1]*weights[1][1] + inputs[2]*weights[1][2] + inputs[3]*weights[1][3] + biases[1],\n",
    "    inputs[0]*weights[2][0] + inputs[1]*weights[2][1] + inputs[2]*weights[2][2] + inputs[3]*weights[2][3] + biases[2],\n",
    "]\n",
    "\n",
    "print(f\"Statically calculated version: {outputs}\")\n",
    "\n",
    "# As I said, the way above doesn't really scale, but we can make it a little bit better with the use of loops to dynamically do this.\n",
    "outputs = []\n",
    "for neuronWeights, neuronBias in zip(weights, biases):\n",
    "    neuronOutput = 0\n",
    "    for nInput, weight in zip(inputs, neuronWeights):\n",
    "        neuronOutput += nInput * weight\n",
    "    neuronOutput += neuronBias\n",
    "    outputs.append(neuronOutput)\n",
    "        \n",
    "print(f\"Dynamically calculated version: {outputs}\")"
   ],
   "id": "1cc868d75f30280b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statically calculated version: [5.3, 7.6000000000000005, 4.7]\n",
      "Dynamically calculated version: [5.3, 7.6000000000000005, 4.7]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above is technically called a \"fully connected\" neural network - where every neuron in the current layer has a connection to each neuron in the previous layer. \n",
    "\n",
    "The number of neurons you use in each layer is totally up to you, and we'll find out throughout the course of the book what can influence your choices there. "
   ],
   "id": "923debb8b81bbf94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 3: Tensors, Arrays, Vectors\n",
    "\n",
    "I'll assume everyone's familiar with basic python and the differences between and names for the following: a = [1,2] vs b = [[1],[2]] vs c = [[1,2],[3,4,5]]. \n",
    "\n",
    "In this above example, a and b can be arrays, while c cannot because it is not \"homologous.\" That is because row 1 is of length 2 whereas row 2 is of length 3, meaning it doesn't follow the form of an array.\n",
    "\n",
    "A matrix is a rectangular array with columns and rows. It can be 2D in the simple case of an (n x m) matrix, lets call it A, where each entry A[i][j], letting i be the row and j be the column, is a single integer. However, we can scale up this matrix in dimensionality by adding more lists in each list. Linear algebra was my favourite class so far, and I geek out about that kind of stuff.\n",
    "\n",
    "I'll show a 2D vs 3D matrix below:  "
   ],
   "id": "a2d17dcbe721efaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.285448Z",
     "start_time": "2025-06-15T20:39:34.281866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# matrixA: a 2D matrix of shape (3, 4)\n",
    "matrixA = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "]\n",
    "\n",
    "# Let's print the entry at row 0 and column 1. \n",
    "print(f\"The entry at row 0 and column 1 is: {matrixA[0][1]}\")\n",
    "\n",
    "# matrixB: a 3D matrix of shape (3, 2, 4)\n",
    "matrixB = [\n",
    "    [[1,2,3,4],\n",
    "     [5,6,7,8]],\n",
    "    [[9,10,11,12],\n",
    "     [13,14,15,16]],\n",
    "    [[17,18,19,20],\n",
    "     [21,22,23,24]]\n",
    "]\n",
    "\n",
    "# Now we can get the entry at the entry i = 0, j = 1, k = 2. I wish I could word that better.\n",
    "print(f\"The entry at i = 0, j = 1, k = 2 is: {matrixB[0][1][2]}\")"
   ],
   "id": "8d38277127e86d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry at row 0 and column 1 is: 2\n",
      "The entry at i = 0, j = 1, k = 2 is: 7\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, about tensors:\n",
    "> A tensor is an object that can be represented as an array.\n",
    "\n",
    "Not all tensors are arrays, every array can be viewed as a tensor. For the purposes of this work, we're told to just view them as one and the same.\n",
    "\n",
    "Lastly, about vectors:\n",
    "> Vectors are just 1D lists in python."
   ],
   "id": "7c637961ebcb21e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: Dot Product and Vector Addition\n",
    "\n",
    "Both dot products and cross products are ways to do vector multiplication. The difference is, the dot product results in a scalar, whereas the cross product results in a vector.\n",
    "\n",
    "Dot products are technically just element-wise multiplication, where both vectors must be of the same size. I'll show an example below:"
   ],
   "id": "3b337fb1e0843f58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.288732Z",
     "start_time": "2025-06-15T20:39:34.286406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given vectors X and Y in a 2D space\n",
    "x = [1, 2]\n",
    "y = [3, 4]\n",
    "\n",
    "# The dot product is therefore:\n",
    "dP = x[0]*y[0] + x[1]*y[1]"
   ],
   "id": "81f911efb2fceed7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 5: A Single Neuron with Numpy\n",
    "\n",
    "This section is a re-creation of section 1, but it's done with numpy instead of manually coding it out. That means we're creating a single neuron to operate on.\n",
    "\n",
    "Let's see how much more efficient we can make it. "
   ],
   "id": "5b6b0c2c533eaf9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.293289Z",
     "start_time": "2025-06-15T20:39:34.290106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = [1.0, 2.1, 3.2, 4.3]\n",
    "weights = [.3, .4, .7, .2]\n",
    "bias = 2.0\n",
    "\n",
    "outputs = np.dot(inputs, weights) + bias\n",
    "\n",
    "print(f\"The neuron output is: {outputs}\")"
   ],
   "id": "bb5c2cb3dcfc88a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neuron output is: 6.24\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 6: A Layer of Neurons with Numpy\n",
    "\n",
    "This section is a re-creation of section 2, but it instead uses numpy to make the multiplication operation more efficient.\n",
    "\n",
    "We'll create a dense layer of 3 neurons, each with 4 in-degree. "
   ],
   "id": "a45569db112c38a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.318236Z",
     "start_time": "2025-06-15T20:39:34.314376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = [1.0, 2.1, 3.2, 4.3]\n",
    "weights = [[0.3, 0.2, 0.4, 0.1],\n",
    "           [0.9, 0.1, 0.8, 0.6],\n",
    "           [.7, 0.2, 0.1, 0.2]]\n",
    "biases = [0.3, 1.2, 2.0]\n",
    "\n",
    "outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(outputs)"
   ],
   "id": "bcaa0006a980dc75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.73 7.45 4.3 ]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 7: A Batch of Data\n",
    "\n",
    "Neural networks typically receive training data in batches! So, what we've been providing so far has been one sample. The reason why we typically train in batches is since a batch is a collection of multiple samples, which has the effect of making the model more generalizable across the whole dataset, versus exactly tuned to the noise of individual samples."
   ],
   "id": "ca3e6a9f3d42a51d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 8: Matrix Products & Transpositions\n",
    "\n",
    "A matrix product takes two matrices and does dot products on all the possible combinations of rows in one matrix and columns in the other. For this to work, the dimensionality of the two matrices needs to be (n x m) and (m x p) where the width of the first matrix needs to coincide with the height of the second matrix.\n",
    "\n",
    "We can also carry out matrix products on vectors, called the row and column vector, that we treat as a (1 x m) or (m x 1) matrix, respectively. This then produces a matrix of size (1 x 1).\n",
    "\n",
    "By the relation of dot products and matrices, we know that:\n",
    "> a * b = ab.T # Where the \".T\" means transpose.\n",
    "\n",
    "A transposition means the rows and columns are flipped. I'll provide a little example below"
   ],
   "id": "753211639372aedf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.322072Z",
     "start_time": "2025-06-15T20:39:34.319586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matrixA = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "]\n",
    "\n",
    "matrixATranspose = [\n",
    "    [1, 5],\n",
    "    [2, 6],\n",
    "    [3, 7],\n",
    "    [4, 8]\n",
    "]"
   ],
   "id": "d750d83f1ffb91a7",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, with this knowledge, we can use Numpy to actually do this vector multiplication.\n",
    "\n",
    "Side note: Numpy does not have separate methods for matrix or dot product, they're just both referred to as \"np.dot(a, b).\""
   ],
   "id": "728677c5ecdc45ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.328910Z",
     "start_time": "2025-06-15T20:39:34.325744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([4,5,6]).T\n",
    "\n",
    "# Now our multiplication follows the form Output = ab.T\n",
    "mMult = np.dot(x, y)\n",
    "print(mMult)"
   ],
   "id": "1f852e09c210c5d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 9: A Layer of Neurons & Batch of Data with Numpy\n",
    "\n",
    "This is effectively the culmination of this entire chapter, where we'll create a layer of neurons that we'll feel a batch of data -- and it'll be easy, thanks to Numpy."
   ],
   "id": "37e0deb5adc4de66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T20:39:34.333377Z",
     "start_time": "2025-06-15T20:39:34.330096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = [[1.0, 2.0, 3.0, 4.0],\n",
    "          [5.0 , 6.0, 7.0, 8.0],\n",
    "          [9.0, 10.0, 11.0, 12.0]]\n",
    "weights = [[.3, .4, .8, .7],\n",
    "           [.4, .5, .9, .8],\n",
    "           [.5, .6, .0, .9]]\n",
    "biases = [6.2, 7.3, 0.5]\n",
    "\n",
    "layerOutputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "print(layerOutputs)"
   ],
   "id": "7ee2f964768a198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.5 14.6  5.8]\n",
      " [21.3 25.  13.8]\n",
      " [30.1 35.4 21.8]]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The above may seem meaningless, and it kind of is? But it's the first full set of predictions that we've made given an input. That is cool, and it'll be even cooler once we actually use it to predict something meaningful! \n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukieła!"
   ],
   "id": "54d5e1a1cf2071dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
