{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 12: Validation Data",
   "id": "8c44557766d917bc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-21T20:48:57.277686Z",
     "start_time": "2025-06-21T20:48:56.847922Z"
    }
   },
   "source": [
    "# Preface: Install necessary packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from timeit import timeit\n",
    "from resources.classes import DenseLayer, ReLU, SoftMax, Loss, CategoricalCrossEntropy, SoftMaxCategoricalCrossEntropy, SGD, AdaGrad, RMSProp, Adam"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In chapter ten, what we did was manually select the hyperparameters that led to better results on the test dataset. However, by doing that, we're optimizing our model for the test dataset. Given our test and train dataset, we were training the model on the train dataset and calculating loss on the test dataset, then evaluating it on it too. That may cause overfitting, just now on the test dataset. \n",
    "\n",
    "To avoid the issue above, we use a third dataset for hyperparameter tuning: the validation dataset. The validation dataset can either be a totally out-of-sample dataset (meaning totally disjoint from both the train & train dataset), or there are two ways to do it if you're short on data.\n",
    "1. Temporarily split the training data into a smaller training dataset and validation dataset for hyperparameter tuning. Once you figure out the optimal hyperparameters, train the model on the whole dataset.\n",
    "2. Use a process called cross-validation. It's primarily used when we have a small training dataset and cannot afford any data for validation. We do this process by splitting our data into a number of small parts -- let's say 5. We now train the model on the first four and validate on the last one. We then do so repeatedly until we have individually used each part for validation. That is called k-fold cross-validation, and it has the benefit of not loosing any training data.\n",
    "\n",
    "Just like last chapter, this one was very short! See you in the next one. \n",
    "\n",
    "### Anyways, that's it for this chapter! Thanks for following along with my annotations of *Neural Networks from Scratch* by Kinsley and Kukie≈Ça!"
   ],
   "id": "f7bf3d5e5a27445"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
